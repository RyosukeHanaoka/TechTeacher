{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObK/rnGUvMiSW417zYmYJG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyosukeHanaoka/TechTeacher/blob/main/Attention%20map\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdoWzha7ueM4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "package_path = '../input/visiontransformerpytorch121/VisionTransformer-Pytorch'\n",
        "sys.path.append(package_path)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "from vision_transformer_pytorch import VisionTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])"
      ],
      "metadata": {
        "id": "FGNu6f_6ujbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Helper functions\n",
        "# ====================================================\n",
        "def load_state(model_path):\n",
        "    state_dict = torch.load(model_path)['model']\n",
        "    state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n",
        "    state_dict = {k[6:] if k.startswith('model.') else k: state_dict[k] for k in state_dict.keys()}\n",
        "\n",
        "    return state_dict"
      ],
      "metadata": {
        "id": "xo1vsKYsum_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_map(img, get_mask=False):\n",
        "    x = transform(img)\n",
        "    x.size()\n",
        "\n",
        "    logits, att_mat = model(x.unsqueeze(0))\n",
        "\n",
        "    att_mat = torch.stack(att_mat).squeeze(1)\n",
        "\n",
        "    # Average the attention weights across all heads.\n",
        "    att_mat = torch.mean(att_mat, dim=1)\n",
        "\n",
        "    # To account for residual connections, we add an identity matrix to the\n",
        "    # attention matrix and re-normalize the weights.\n",
        "    residual_att = torch.eye(att_mat.size(1))\n",
        "    aug_att_mat = att_mat + residual_att\n",
        "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
        "\n",
        "    # Recursively multiply the weight matrices\n",
        "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
        "    joint_attentions[0] = aug_att_mat[0]\n",
        "\n",
        "    for n in range(1, aug_att_mat.size(0)):\n",
        "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
        "\n",
        "    v = joint_attentions[-1]\n",
        "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
        "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
        "    if get_mask:\n",
        "        result = cv2.resize(mask / mask.max(), img.size)\n",
        "    else:\n",
        "        mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n",
        "        result = (mask * img).astype(\"uint8\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def plot_attention_map(original_img, att_map):\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "    ax1.set_title('Original')\n",
        "    ax2.set_title('Attention Map Last Layer')\n",
        "    _ = ax1.imshow(original_img)\n",
        "    _ = ax2.imshow(att_map)"
      ],
      "metadata": {
        "id": "Ps3jE71Ruq_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer.from_name('ViT-B_16', num_classes=5)\n",
        "state = load_state('../input/cassava-vit-b-16/ViT-B_16_fold0.pth')\n",
        "model.load_state_dict(state)"
      ],
      "metadata": {
        "id": "CL3vOGm6uxMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = pd.read_json('../input/cassava-leaf-disease-classification/label_num_to_disease_map.json',\n",
        "                         orient='index')\n",
        "\n",
        "display(label_map)"
      ],
      "metadata": {
        "id": "wtAUU_CHu0pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img1 = Image.open(\"../input/cassava-vit-b-16/train-cbb-114.jpg\")\n",
        "img2 = Image.open(\"../input/cassava-vit-b-16/train-cbb-44.jpg\")\n",
        "\n",
        "result1 = get_attention_map(img1)\n",
        "result2 = get_attention_map(img2)"
      ],
      "metadata": {
        "id": "s8cHAjxfu6xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention_map(img1, result1)"
      ],
      "metadata": {
        "id": "PPBZ5Jxhu8h8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}