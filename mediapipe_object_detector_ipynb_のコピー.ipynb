{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyosukeHanaoka/TechTeacher/blob/main/mediapipe_object_detector_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOlNZgOafs5b"
      },
      "source": [
        "Project: /mediapipe/_project.yaml\n",
        "Book: /mediapipe/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Object detection model customization guide\n",
        "\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzM-FLsJKwij"
      },
      "source": [
        "MediaPipe オブジェクト検出ソリューションには、アプリケーションの機械学習 (ML) にすぐに使用できるモデルがいくつか用意されています。 ただし、提供されたモデルでカバーされていないオブジェクトを検出する必要がある場合は、独自のデータと MediaPipe Model Maker を使用して提供されたモデルのいずれかをカスタマイズできます。 このモデル変更ツールは、提供されたデータを使用してモデルを再構築します。 この方法は、新しいモデルをトレーニングするよりも速く、特定のアプリケーションにとってより役立つモデルを生成できます。\n",
        "\n",
        "次のセクションでは、Model Maker を使用して、独自のデータを使用してオブジェクト検出用の事前構築モデルを再トレーニングし、その後 MediaPipe Object Detector で使用できるようにする方法を示します。 この例では、画像内の Android フィギュアを検出するために汎用オブジェクト検出モデルを再トレーニングします。\n",
        "\n",
        "The MediaPipe object detection solution provides several models you can use immediately for machine learning (ML) in your application. However, if you need to detect objects not covered by the provided models, you can customize any of the provided models with your own data and MediaPipe Model Maker. This model modification tool rebuilds the model using data you provide. This method is faster than training a new model and can produce a model that is more useful for your specific application.\n",
        "\n",
        "The following sections show you how to use Model Maker to retrain a pre-built model for object detection with your own data, which you can then use with the MediaPipe [Object Detector](https://developers.google.com/mediapipe/solutions/vision/object_detector). The example retrains a general purpose object detection model to detect android figurines in images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_9q7fppLbPA"
      },
      "source": [
        "## 設定\n",
        "\n",
        "このセクションでは、モデルを再トレーニングするために開発環境をセットアップするための主要な手順について説明します。 これらの手順では、[Google Colab](https://colab.research.google.com/) を使用してモデルを更新する方法について説明します。また、独自の開発環境で Python を使用することもできます。 MediaPipe を使用するための開発環境のセットアップに関する一般情報 (プラットフォームのバージョン要件など) については、[Python のセットアップ ガイド](https://developers.google.com/mediapipe/solutions/setup_python) を参照してください。\n",
        "\n",
        "**注意:** この MediaPipe ソリューション プレビューは初期リリースです。 [詳細](https://developers.google.com/mediapipe/solutions/about)。\n",
        "\n",
        "モデルをカスタマイズするためのライブラリをインストールするには、次のコマンドを実行します。\n",
        "\n",
        "## Setup\n",
        "\n",
        "This section describes key steps for setting up your development environment to retrain a model. These instructions describe how to update a model using [Google Colab](https://colab.research.google.com/), and you can also use Python in your own development environment. For general information on setting up your development environment for using MediaPipe, including platform version requirements, see the [Setup guide for Python](https://developers.google.com/mediapipe/solutions/setup_python).\n",
        "\n",
        "**Attention:** This MediaPipe Solutions Preview is an early release. [Learn more](https://developers.google.com/mediapipe/solutions/about).\n",
        "\n",
        "To install the libraries for customizing a model, run the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMSCcHh1LScM"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXvZgLPLh6n"
      },
      "source": [
        "次のコードを使用して、必要な Python クラスをインポートします。\n",
        "\n",
        "Use the following code to import the required Python classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oazmbPzKHYFq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import object_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zP1AkaRL72Z"
      },
      "source": [
        "## データの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVnSV0ZQMA5-"
      },
      "source": [
        "オブジェクト検出用にモデルを再トレーニングするには、完成したモデルで識別できるようにするアイテムまたはクラスを含むデータセットが必要です。 これを行うには、パブリック データセットをトリミングしてユースケースに関連するクラスのみにするか、独自のデータセットをコンパイルするか、またはその両方を組み合わせます。データセットは、新しいモデルのトレーニングに必要なデータセットよりも大幅に小さくなる可能性があります。 たとえば、多くの参照モデルのトレーニングに使用される COCO データセットには、91 クラスのオブジェクトを含む数十万の画像が含まれています。 Model Maker を使用した転移学習は、推論精度の目標に応じて、より小さなデータセットを使用して既存のモデルを再トレーニングしても、良好なパフォーマンスを維持できます。 これらの手順では、2 種類のアンドロイド フィギュア、つまり 2 つのクラス、合計 62 個のトレーニング画像を含む小さなデータセットを使用します。\n",
        "\n",
        "サンプル データセットをダウンロードするには、次のコードを使用します。\n",
        "\n",
        "Retraining a model for object detection requires a dataset that includes the items, or classes, that you want the completed model to be able to identify. You can do this by trimming down a public dataset to only the classes that are relevant to your usecase, compiling your own dataset, or some combination of both, The dataset can be significantly smaller than what would be required to train a new model. For example, the [COCO](https://cocodataset.org/) dataset used to train many reference models contains hundreds of thousands of images with 91 classes of objects. Transfer learning with Model Maker can retrain an existing model with a smaller dataset and still perform well, depending on your inference accuracy goals. These instructions use a smaller dataset containing 2 types of android figurines, or 2 classes, with 62 total training images.\n",
        "\n",
        "To download the example dataset, use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz3-eHe07FEX"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/mediapipe-tasks/object_detector/android_figurine.zip\n",
        "!unzip android_figurine.zip\n",
        "train_dataset_path = \"android_figurine/train\"\n",
        "validation_dataset_path = \"android_figurine/validation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEVcacUj7L1l"
      },
      "source": [
        "This code stores the dataset at the directory location `android_figurine`. The directory contains two subdirectories for the training and validation datasets, located in `android_figurine/train` and `android_figurine/validation` respectively. Each of the train and validation datasets follow the COCO Dataset format described below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbIAPjPUMHiK"
      },
      "source": [
        "### サポートされているデータセット形式\n",
        "Model Maker Object Detection API は、次のデータセット形式の読み取りをサポートしています。\n",
        "\n",
        "#### COCO 形式\n",
        "\n",
        "COCO データセット形式には、すべての画像を保存する「data」ディレクトリと、すべての画像のオブジェクト注釈を含む単一の「labels.json」ファイルがあります。\n",
        "\n",
        "### Supported dataset formats\n",
        "Model Maker Object Detection API supports reading the following dataset formats:\n",
        "\n",
        "#### COCO format\n",
        "\n",
        "The COCO dataset format has a `data` directory which stores all of the images and a single `labels.json` file which contains the object annotations for all images.\n",
        "```\n",
        "<dataset_dir>/\n",
        "  data/\n",
        "    <img0>.<jpg/jpeg>\n",
        "    <img1>.<jpg/jpeg>\n",
        "    ...\n",
        "  labels.json\n",
        "```\n",
        "where `labels.json` is formatted as:\n",
        "```\n",
        "{\n",
        "  \"categories\":[\n",
        "    {\"id\":1, \"name\":<cat1_name>},\n",
        "    ...\n",
        "  ],\n",
        "  \"images\":[\n",
        "    {\"id\":0, \"file_name\":\"<img0>.<jpg/jpeg>\"},\n",
        "    ...\n",
        "  ],\n",
        "  \"annotations\":[\n",
        "    {\"id\":0, \"image_id\":0, \"category_id\":1, \"bbox\":[x-top left, y-top left, width, height]},\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### PASCAL VOC format\n",
        "\n",
        "The PASCAL VOC dataset format also has a `data` directory which stores all of the images, however the annotations are split up per image into corresponding xml files in the `Annotations` directory.\n",
        "```\n",
        "<dataset_dir>/\n",
        "  data/\n",
        "    <file0>.<jpg/jpeg>\n",
        "    ...\n",
        "  Annotations/\n",
        "    <file0>.xml\n",
        "    ...\n",
        "```\n",
        "where the xml files are formatted as:\n",
        "```\n",
        "<annotation>\n",
        "  <filename>file0.jpg</filename>\n",
        "  <object>\n",
        "    <name>kangaroo</name>\n",
        "    <bndbox>\n",
        "      <xmin>233</xmin>\n",
        "      <ymin>89</ymin>\n",
        "      <xmax>386</xmax>\n",
        "      <ymax>262</ymax>\n",
        "    </bndbox>\n",
        "  </object>\n",
        "  <object>\n",
        "    ...\n",
        "  </object>\n",
        "  ...\n",
        "</annotation>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TPn8Mb_aJb"
      },
      "source": [
        "### データセットを確認する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L64U7mgPNKec"
      },
      "source": [
        "「labels.json」ファイルからカテゴリを出力して、データセットの内容を確認します。 カテゴリは合計 3 つあるはずです。 インデックス 0 は常に、データセット内で使用されない可能性がある「バックグラウンド」クラスに設定されます。 `android` と `pig_android` という 2 つの非バックグラウンド カテゴリがあるはずです。\n",
        "\n",
        "Verify the dataset content by printing the categories from the `labels.json` file. There should be 3 total categories. Index 0 is always set to be the `background` class which may be unused in the dataset. There should be two non-background categories of `android` and `pig_android`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f_Z-TAwNK3n"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(train_dataset_path, \"labels.json\"), \"r\") as f:\n",
        "  labels_json = json.load(f)\n",
        "for category_item in labels_json[\"categories\"]:\n",
        "  print(f\"{category_item['id']}: {category_item['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr-7QJ05PmyS"
      },
      "source": [
        "データセットをより深く理解するには、いくつかのサンプル画像をその境界ボックスとともにプロットします。\n",
        "\n",
        "To better understand the dataset, plot a couple of example images along with their bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kTw3uodPl7-"
      },
      "outputs": [],
      "source": [
        "#@title トレーニングデータの視覚化\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, text, patheffects\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "def draw_outline(obj):\n",
        "  obj.set_path_effects([patheffects.Stroke(linewidth=4,  foreground='black'), patheffects.Normal()])\n",
        "def draw_box(ax, bb):\n",
        "  patch = ax.add_patch(patches.Rectangle((bb[0],bb[1]), bb[2], bb[3], fill=False, edgecolor='red', lw=2))\n",
        "  draw_outline(patch)\n",
        "def draw_text(ax, bb, txt, disp):\n",
        "  text = ax.text(bb[0],(bb[1]-disp),txt,verticalalignment='top'\n",
        "  ,color='white',fontsize=10,weight='bold')\n",
        "  draw_outline(text)\n",
        "def draw_bbox(ax, annotations_list, id_to_label, image_shape):\n",
        "  for annotation in annotations_list:\n",
        "    cat_id = annotation[\"category_id\"]\n",
        "    bbox = annotation[\"bbox\"]\n",
        "    draw_box(ax, bbox)\n",
        "    draw_text(ax, bbox, id_to_label[cat_id], image_shape[0] * 0.05)\n",
        "def visualize(dataset_folder, max_examples=None):\n",
        "  with open(os.path.join(dataset_folder, \"labels.json\"), \"r\") as f:\n",
        "    labels_json = json.load(f)\n",
        "  images = labels_json[\"images\"]\n",
        "  cat_id_to_label = {item[\"id\"]:item[\"name\"] for item in labels_json[\"categories\"]}\n",
        "  image_annots = defaultdict(list)\n",
        "  for annotation_obj in labels_json[\"annotations\"]:\n",
        "    image_id = annotation_obj[\"image_id\"]\n",
        "    image_annots[image_id].append(annotation_obj)\n",
        "\n",
        "  if max_examples is None:\n",
        "    max_examples = len(image_annots.items())\n",
        "  n_rows = math.ceil(max_examples / 3)\n",
        "  fig, axs = plt.subplots(n_rows, 3, figsize=(24, n_rows*8)) # 3 columns(2nd index), 8x8 for each image\n",
        "  for ind, (image_id, annotations_list) in enumerate(list(image_annots.items())[:max_examples]):\n",
        "    ax = axs[ind//3, ind%3]\n",
        "    img = plt.imread(os.path.join(dataset_folder, \"images\", images[image_id][\"file_name\"]))\n",
        "    ax.imshow(img)\n",
        "    draw_bbox(ax, annotations_list, cat_id_to_label, img.shape)\n",
        "  plt.show()\n",
        "\n",
        "visualize(train_dataset_path, 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANqfl-ghQM74"
      },
      "source": [
        "### データセットを作成する\n",
        "\n",
        "Dataset クラスには、COCO または PASCAL VOC データセットをロードするための 2 つのメソッドがあります。\n",
        "* `Dataset.from_coco_folder`\n",
        "* `Dataset.from_pascal_voc_folder`\n",
        "\n",
        "android_figurines データセットは COCO データセット形式であるため、`from_coco_folder` メソッドを使用して、`train_dataset_path` と `validation_dataset_path` にあるデータセットを読み込みます。 データセットをロードすると、データは指定されたパスから解析され、標準化された [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) 形式に変換され、後で使用するためにキャッシュされます。 同じデータセットの複数のキャッシュを保存しないようにするには、`cache_dir` の場所を作成し、すべてのトレーニングでそれを再利用する必要があります。\n",
        "\n",
        "### Create dataset\n",
        "\n",
        "The Dataset class has two methods for loading in COCO or PASCAL VOC datasets:\n",
        "* `Dataset.from_coco_folder`\n",
        "* `Dataset.from_pascal_voc_folder`\n",
        "\n",
        "Since the android_figurines dataset is in the COCO dataset format, use the `from_coco_folder` method to load the dataset located at `train_dataset_path` and `validation_dataset_path`. When loading the dataset, the data will be parsed from the provided path and converted into a standardized [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format which is cached for later use. You should create a `cache_dir` location and reuse it for all your training to avoid saving multiple caches of the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdyImqyI6s-"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.Dataset.from_coco_folder(train_dataset_path, cache_dir=\"/tmp/od_data/train\")\n",
        "validation_data = object_detector.Dataset.from_coco_folder(validation_dataset_path, cache_dir=\"/tmp/od_data/validation\")\n",
        "print(\"train_data size: \", train_data.size)\n",
        "print(\"validation_data size: \", validation_data.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "det3wYoWRRDs"
      },
      "source": [
        "## 再学習過程\n",
        "\n",
        "データの準備が完了したら、トレーニングデータで定義した新たな物体やクラスを認識するように再学習過程を開始することができる。以下の手順では、前のセクションで準備したデータを使用して画像分類モデルを再トレーニングし、2種類のアンドロイド フィギュアを認識する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9AzF_CGA7mj"
      },
      "source": [
        "### Set retraining options\n",
        "\n",
        "再トレーニングを実行するには、トレーニング データセットのほかに、モデルの出力ディレクトリとモデル アーキテクチャといういくつかの必要な設定があります。 HParams を使用して、出力ディレクトリの export_dir パラメータを指定します。 SupportedModels クラスを使用してモデル アーキテクチャを指定します。 物体検出ソリューションは、次のモデル アーキテクチャをサポートします。\n",
        "* `MobileNet-V2`\n",
        "* `MobileNet-MultiHW-AVG`\n",
        "トレーニング パラメーターのより高度なカスタマイズについては、以下の [ハイパーパラメーター](#hyperparameters) セクションをご覧ください。\n",
        "\n",
        "必要なパラメータを設定するには、次のコードを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZHjWHM1JyiN"
      },
      "outputs": [],
      "source": [
        "spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG\n",
        "hparams = object_detector.HParams(export_dir='exported_model')\n",
        "options = object_detector.ObjectDetectorOptions(\n",
        "    supported_model=spec,\n",
        "    hparams=hparams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Kto1RlCcPj"
      },
      "source": [
        "### 再学習過程\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5bIsWBZCb8d"
      },
      "outputs": [],
      "source": [
        "model = object_detector.ObjectDetector.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuRapoFiRp34"
      },
      "source": [
        "### モデルのパフォーマンスを評価する\n",
        "\n",
        "モデルをトレーニングした後、検証データセットを用いてモデルを評価し、損失率と coco_metrics を出力します。 モデルのパフォーマンスを評価するための最も重要な指標は、通常、平均精度の「AP」ココメトリックです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJvB_nf7RwzJ"
      },
      "outputs": [],
      "source": [
        "loss, coco_metrics = model.evaluate(validation_data, batch_size=4)\n",
        "print(f\"Validation loss: {loss}\")\n",
        "print(f\"Validation coco metrics: {coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax8TkYA9VJUv"
      },
      "source": [
        "## モデルのエクスポート\n",
        "\n",
        "モデルを作成した後、後でデバイス上のアプリケーションで使用できるように、モデルを Tensorflow Lite モデル形式に変換してエクスポートします。 エクスポートには、ラベルマップを含むモデルメタデータも含まれます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWZqnGEKVP13"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model\n",
        "files.download('exported_model/model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcYu5ENbT4T6"
      },
      "source": [
        "## モデルの量子化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIeJjCfWTnBj"
      },
      "source": [
        "モデルを作成した後、後でデバイス上のアプリケーションで使用できるように、モデルを Tensorflow Lite モデル形式に変換してエクスポートします。 エクスポートにはモデルも含まれます。 モデル量子化は、比較的わずかな精度の低下のみでモデル サイズを削減し、予測速度を向上させることができるモデル変更手法です。\n",
        "\n",
        "ガイドのこのセクションでは、モデルに量子化を適用する方法について説明します。 Model Maker は、オブジェクト検出器の 2 つの形式の量子化をサポートしています。\n",
        "1. 量子化対応トレーニング: CPU 使用のための 8 ビット整数精度\n",
        "2. トレーニング後の量子化: GPU使用のための16 ビット浮動小数点精度。\n",
        "\n",
        "Model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy.\n",
        "\n",
        "This section of the guide explains how to apply quantization to your model. Model Maker supports two forms of quantization for object detector:\n",
        "1. Quantization Aware Training: 8 bit integer precision for CPU usage\n",
        "2. Post-Training Quantization: 16 bit floating point precision for GPU usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcB3DRfHWDfs"
      },
      "source": [
        "### 量子化を意識したトレーニング (int8 量子化)\n",
        "量子化対応トレーニング (QAT) は、モデルを完全にトレーニングした後に行われる微調整ステップです。 この手法では、8 ビット整数量子化の精度の低さを考慮して、推論時間量子化をエミュレートするモデルをさらに調整します。 標準 CPU を備えたオンデバイス アプリケーションの場合は、Int8 精度を使用します。 詳細については、[TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/training) ドキュメントを参照してください。\n",
        "\n",
        "量子化対応トレーニングを適用して int8 モデルにエクスポートするには、`QATHParams` 設定を作成し、`quantization_aware_training` メソッドを実行します。 `QATHParams` の詳細な使用法については、以下の **ハイパーパラメータ** セクションを参照してください。\n",
        "\n",
        "### Quantization aware training (int8 quantization)\n",
        "Quantization aware training (QAT) is a fine-tuning step which happens after fully training your model. This technique further tunes a model which emulates inference time quantization in order to account for the lower precision of 8 bit integer quantization. For on-device applications with a standard CPU, use Int8 precision. For more information, see the [TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/training) documentation.\n",
        "\n",
        "To apply quantization aware training and export to an int8 model, create a `QATHParams` configuration and run the `quantization_aware_training` method. See the **Hyperparameters** section below on detailed usage of `QATHParams`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7nRSQT9WCS-"
      },
      "outputs": [],
      "source": [
        "qat_hparams = object_detector.QATHParams(learning_rate=0.3, batch_size=4, epochs=10, decay_steps=6, decay_rate=0.96)\n",
        "model.quantization_aware_training(train_data, validation_data, qat_hparams=qat_hparams)\n",
        "qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n",
        "print(f\"QAT validation loss: {qat_loss}\")\n",
        "print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT7grgHOW048"
      },
      "source": [
        "QAT ステップでは、トレーニングのパラメーターを調整するために複数の実行が必要になることがよくあります。 `create` メソッドを使用してモデル トレーニングを再実行する必要がないようにするには、QAT を再度実行するために、`restore_float_ckpt` メソッドを使用してモデルの状態を完全にトレーニングされた浮動小数点モデルに戻します (`create` メソッドの実行後)。\n",
        "\n",
        "The QAT step often requires multiple runs to tune the parameters of training. To avoid having to rerun model training using the `create` method, use the `restore_float_ckpt` method to restore the model state back to the fully trained float model(After running the `create` method) in order to run QAT again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHDPWbIaXjR4"
      },
      "outputs": [],
      "source": [
        "new_qat_hparams = object_detector.QATHParams(learning_rate=0.9, batch_size=4, epochs=15, decay_steps=5, decay_rate=0.96)\n",
        "model.restore_float_ckpt()\n",
        "model.quantization_aware_training(train_data, validation_data, qat_hparams=new_qat_hparams)\n",
        "qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n",
        "print(f\"QAT validation loss: {qat_loss}\")\n",
        "print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfWo6TVpWJfr"
      },
      "source": [
        "最後に、export_model を使用して int8 量子化モデルにエクスポートします。 export_model 関数は、quantization_aware_training が実行されたかどうかに応じて、float32 モデルまたは int8 モデルのいずれかに自動的にエクスポートします。\n",
        "\n",
        "Finally, us the `export_model` to export to an int8 quantized model. The `export_model` function will automatically export to either float32 or int8 model depending on whether `quantization_aware_training` was run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsixePxCWJDp"
      },
      "outputs": [],
      "source": [
        "model.export_model('model_int8_qat.tflite')\n",
        "!ls -lh exported_model\n",
        "files.download('exported_model/model_int8_qat.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO8nZR3Cgx8_"
      },
      "source": [
        "### トレーニング後の量子化 (fp16 量子化)\n",
        "\n",
        "トレーニング後のモデルの量子化は、比較的わずかな精度の低下のみでモデルのサイズを削減し、予測速度を向上させることができるモデル変更手法です。 このアプローチでは、たとえば 32 ビット浮動小数点数を 16 ビット浮動小数点数に変換することにより、モデルによって処理されるデータのサイズが削減されます。 GPU の使用には Float16 量子化が推奨されます。 詳細については、[TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) ドキュメントを参照してください。\n",
        "\n",
        "まず、MediaPipe Model Maker 量子化モジュールをインポートします。\n",
        "\n",
        "### Post-training quantization (fp16 quantization)\n",
        "\n",
        "Post-training model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy. This approach reduces the size of the data processed by the model, for example by transforming 32-bit floating point numbers to 16-bit floats. Float16 quantization is reccomended for GPU usage. For more information, see the [TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) documentation.\n",
        "\n",
        "First, import the MediaPipe Model Maker quantization module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo7cQ_N-ZE8A"
      },
      "outputs": [],
      "source": [
        "from mediapipe_model_maker import quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE8j5cloZTo-"
      },
      "source": [
        "for_float16() クラス メソッドを使用して QuantizationConfig オブジェクトを定義します。 この構成は、32 ビット浮動小数点数の代わりに 16 ビット浮動小数点数を使用するようにトレーニングされたモデルを変更します。 QuantizationConfig クラスの追加パラメーターを設定することで、量子化プロセスをさらにカスタマイズできます。\n",
        "\n",
        "Define a QuantizationConfig object using the `for_float16()` class method. This configuration modifies a trained model to use 16-bit floating point numbers instead of 32-bit floating point numbers. You can further customize the quantization process by setting additional parameters for the QuantizationConfig class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbwc3g_Fa3dv"
      },
      "outputs": [],
      "source": [
        "quantization_config = quantization.QuantizationConfig.for_float16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTrkDXi8bM_L"
      },
      "source": [
        "追加の quantization_config オブジェクトを使用してモデルをエクスポートし、トレーニング後の量子化を適用します。 以前に quantization_aware_training を実行したことがある場合は、まず、restore_float_ckpt を使用してモデルを float モデルに変換し直す必要があることに注意してください。\n",
        "\n",
        "Export the model using the additional quantization_config object to apply post-training quantization. Note that if you previously ran `quantization_aware_training`, you must first convert the model back to a float model by using `restore_float_ckpt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmzEu_AjbMPI"
      },
      "outputs": [],
      "source": [
        "model.restore_float_ckpt()\n",
        "model.export_model(model_name=\"model_fp16.tflite\", quantization_config=quantization_config)\n",
        "!ls -lh exported_model\n",
        "files.download('exported_model/model_fp16.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npaRBUB3ZevY"
      },
      "source": [
        "##ハイパーパラメータ\n",
        "ObjectDetectorOptions クラスを使用してモデルをさらにカスタマイズできます。このクラスには、SupportedModels、ModelOptions、および HParams の 3 つのパラメーターがあります。\n",
        "\n",
        "SupportedModels enum クラスを使用して、トレーニングに使用するモデル アーキテクチャを指定します。 次のモデル アーキテクチャがサポートされています。\n",
        "\n",
        "モバイルネット_V2\n",
        "モバイルネット_V2_I320\n",
        "MOBILENET_MULTI_AVG\n",
        "MOBILENET_MULTI_AVG_I384\n",
        "HParams クラスを使用して、モデルのトレーニングと保存に関連する他のパラメーターをカスタマイズします。\n",
        "\n",
        "learning_rate: 勾配降下トレーニングに使用する学習率。 デフォルトは 0.3 です。\n",
        "batch_size: トレーニングのバッチ サイズ。 デフォルトは 8 です。\n",
        "エポック: データセットに対するトレーニング反復の数。 デフォルトは 30 です。\n",
        "cosine_decay_epochs: コサイン減衰学習率のエポック数。 詳細については、「tf.keras.optimizers.schedules.CosineDecay」を参照してください。 デフォルトは None で、これはエポックに設定するのと同じです。\n",
        "cosine_decay_alpha: コサイン減衰学習率のアルファ値。 詳細については、「tf.keras.optimizers.schedules.CosineDecay」を参照してください。 デフォルトは 1.0 で、コサイン減衰がないことを意味します。\n",
        "ModelOptions クラスを使用して、モデル自体に関連するパラメーターをカスタマイズします。\n",
        "\n",
        "l2_weight_decay: tf.keras.regulators.L2 で使用される L2 正則化ペナルティ。 デフォルトは 3e-5 です。\n",
        "QATHParams クラスを使用して、量子化対応トレーニングのトレーニング パラメーターをカスタマイズします。\n",
        "\n",
        "learning_rate: 勾配降下 QAT に使用する学習率。 デフォルトは 0.3 です。\n",
        "batch_size: QAT のバッチ サイズ。 デフォルトは 8\n",
        "エポック: データセットに対するトレーニング反復の数。 デフォルトは 15 です。\n",
        "decay_steps: 指数関数的減衰の学習率減衰ステップ。 詳細については、「tf.keras.optimizers.schedules.ExponentialDecay」を参照してください。 デフォルトは 8\n",
        "decay_rate: 指数関数的減衰の学習率減衰率。 詳細については、「tf.keras.optimizers.schedules.ExponentialDecay」を参照してください。 デフォルトは 0.96 です。\n",
        "\n",
        "## Hyperparameters\n",
        "You can further customize the model using the ObjectDetectorOptions class, which has three parameters for `SupportedModels`, `ModelOptions`, and `HParams`.\n",
        "\n",
        "Use the `SupportedModels` enum class to specify the model architecture to use for training. The following model architectures are supported:\n",
        "* MOBILENET_V2\n",
        "* MOBILENET_V2_I320\n",
        "* MOBILENET_MULTI_AVG\n",
        "* MOBILENET_MULTI_AVG_I384\n",
        "\n",
        "Use the `HParams` class to customize other parameters related to training and saving the model:\n",
        "* `learning_rate`: Learning rate to use for gradient descent training. Defaults to 0.3.\n",
        "* `batch_size`: Batch size for training. Defaults to 8.\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 30.\n",
        "* `cosine_decay_epochs`: The number of epochs for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to None, which is equivalent to setting it to `epochs`.\n",
        "* `cosine_decay_alpha`: The alpha value for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to 1.0, which means no cosine decay.\n",
        "\n",
        "Use the `ModelOptions` class to customize parameters related to the model itself:\n",
        "* `l2_weight_decay`: L2 regularization penalty used in [tf.keras.regularizers.L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2). Defaults to 3e-5.\n",
        "\n",
        "Uset the `QATHParams` class to customize training parameters for Quantization Aware Training:\n",
        "* `learning_rate`: Learning rate to use for gradient descent QAT. Defaults to 0.3.\n",
        "* `batch_size`: Batch size for QAT. Defaults to 8\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 15.\n",
        "* `decay_steps`: Learning rate decay steps for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 8\n",
        "* `decay_rate`: Learning rate decay rate for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 0.96."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HCrUl8z6liX"
      },
      "source": [
        "## ベンチマーク\n",
        "以下は、サポートされているモデル アーキテクチャのベンチマーク結果の概要です。 これらのモデルは、このノートブックと同じ Android フィギュア データセットでトレーニングおよび評価されました。 モデルのベンチマーク結果を検討する際には、留意すべき重要な注意点がいくつかあります。\n",
        "* Android フィギュア データセットは、62 個のトレーニング サンプルと 10 個の検証サンプルを含む小規模でシンプルなデータセットです。 データセットは非常に小さいため、トレーニング プロセスの差異によりメトリクスが大幅に変化する可能性があります。 このデータセットはデモ目的で提供されており、モデルのパフォーマンスを向上させるためにさらに多くのデータ サンプルを収集することをお勧めします。\n",
        "* float32 モデルはデフォルトの HParams でトレーニングされ、int8 モデルの QAT ステップは `QATHParams(learning_rate=0.1,batch_size=4,epochs=30,decay_rate=1)` で実行されました。\n",
        "* 独自のデータセットの場合、最良の結果を得るには、HParams と QATHParams の両方の値を調整する必要がある可能性があります。 トレーニング パラメーターの構成の詳細については、上記の [Hyperparameters](#hyperparameters) セクションを参照してください。\n",
        "* すべての遅延数値は Pixel 6 でベンチマークされています。\n",
        "\n",
        "## Benchmarking\n",
        "Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same android figurines dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n",
        "* The android figurines dataset is a small and simple dataset with 62 training examples and 10 validation examples. Since the dataset is quite small, metrics may vary drastically due to variances in the training process. This dataset was provided for demo purposes and it is recommended to collect more data samples for better performing models.\n",
        "* The float32 models were trained with the default HParams, and the QAT step for the int8 models was run with `QATHParams(learning_rate=0.1, batch_size=4, epochs=30, decay_rate=1)`.\n",
        "* For your own dataset, you will likely need to tune values for both HParams and QATHParams in order to achieve the best results. See the [Hyperparameters](#hyperparameters) section above for more information on configuring training parameters.\n",
        "* All latency numbers are benchmarked on the Pixel 6.\n",
        "\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<col>\n",
        "<col>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<tr>\n",
        "<th rowspan=\"2\">Model architecture</th>\n",
        "<th rowspan=\"2\">Input Image Size</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Test AP</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">CPU Latency</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Model Size</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNetV2</td>\n",
        "<td>256x256</td>\n",
        "<td>88.4%</td>\n",
        "<td>73.5%</td>\n",
        "<td>48ms</td>\n",
        "<td>16ms</td>\n",
        "<td>11MB</td>\n",
        "<td>3.2MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNetV2 I320</td>\n",
        "<td>320x320</td>\n",
        "<td>89.1%</td>\n",
        "<td>75.5%</td>\n",
        "<td>75ms</td>\n",
        "<td>33.38ms</td>\n",
        "<td>10MB</td>\n",
        "<td>3.3MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG</td>\n",
        "<td>256x256</td>\n",
        "<td>88.5%</td>\n",
        "<td>70.0%</td>\n",
        "<td>56ms</td>\n",
        "<td>19ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG I384</td>\n",
        "<td>384x384</td>\n",
        "<td>92.7%</td>\n",
        "<td>73.4%</td>\n",
        "<td>238ms</td>\n",
        "<td>41ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOCPzKohXxy6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}