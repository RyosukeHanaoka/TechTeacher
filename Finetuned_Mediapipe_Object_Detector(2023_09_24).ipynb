{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyosukeHanaoka/TechTeacher/blob/main/Finetuned_Mediapipe_Object_Detector(2023_09_24).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOlNZgOafs5b"
      },
      "source": [
        "Project: /mediapipe/_project.yaml\n",
        "Book: /mediapipe/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Object detection model customization guide\n",
        "\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzM-FLsJKwij"
      },
      "source": [
        "MediaPipe オブジェクト検出ソリューションには、アプリケーションの機械学習 (ML) にすぐに使用できるモデルがいくつか用意されています。 ただし、提供されたモデルでカバーされていないオブジェクトを検出する必要がある場合は、独自のデータと MediaPipe Model Maker を使用して提供されたモデルのいずれかをカスタマイズできます。 このモデル変更ツールは、提供されたデータを使用してモデルを再構築します。 この方法は、新しいモデルをトレーニングするよりも速く、特定のアプリケーションにとってより役立つモデルを生成できます。\n",
        "\n",
        "次のセクションでは、Model Maker を使用して、独自のデータを使用してオブジェクト検出用の事前構築モデルを再トレーニングし、その後 MediaPipe Object Detector で使用できるようにする方法を示します。 この例では、画像内の Android フィギュアを検出するために汎用オブジェクト検出モデルを再トレーニングします。\n",
        "\n",
        "The MediaPipe object detection solution provides several models you can use immediately for machine learning (ML) in your application. However, if you need to detect objects not covered by the provided models, you can customize any of the provided models with your own data and MediaPipe Model Maker. This model modification tool rebuilds the model using data you provide. This method is faster than training a new model and can produce a model that is more useful for your specific application.\n",
        "\n",
        "The following sections show you how to use Model Maker to retrain a pre-built model for object detection with your own data, which you can then use with the MediaPipe [Object Detector](https://developers.google.com/mediapipe/solutions/vision/object_detector). The example retrains a general purpose object detection model to detect android figurines in images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_9q7fppLbPA"
      },
      "source": [
        "## 設定\n",
        "\n",
        "このセクションでは、モデルを再トレーニングするために開発環境をセットアップするための主要な手順について説明します。 これらの手順では、[Google Colab](https://colab.research.google.com/) を使用してモデルを更新する方法について説明します。また、独自の開発環境で Python を使用することもできます。 MediaPipe を使用するための開発環境のセットアップに関する一般情報 (プラットフォームのバージョン要件など) については、[Python のセットアップ ガイド](https://developers.google.com/mediapipe/solutions/setup_python) を参照してください。\n",
        "\n",
        "**注意:** この MediaPipe ソリューション プレビューは初期リリースです。 [詳細](https://developers.google.com/mediapipe/solutions/about)。\n",
        "\n",
        "モデルをカスタマイズするためのライブラリをインストールするには、次のコマンドを実行します。\n",
        "\n",
        "## Setup\n",
        "\n",
        "This section describes key steps for setting up your development environment to retrain a model. These instructions describe how to update a model using [Google Colab](https://colab.research.google.com/), and you can also use Python in your own development environment. For general information on setting up your development environment for using MediaPipe, including platform version requirements, see the [Setup guide for Python](https://developers.google.com/mediapipe/solutions/setup_python).\n",
        "\n",
        "**Attention:** This MediaPipe Solutions Preview is an early release. [Learn more](https://developers.google.com/mediapipe/solutions/about).\n",
        "\n",
        "To install the libraries for customizing a model, run the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GMSCcHh1LScM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa3f041-63f4-4b86-d18c-db280db4dbb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: mediapipe-model-maker in /usr/local/lib/python3.10/dist-packages (0.2.1.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.4.0)\n",
            "Requirement already satisfied: mediapipe>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.10.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.8.0.76)\n",
            "Requirement already satisfied: tensorflow>=2.10 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.21.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.9.2)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.14.0)\n",
            "Requirement already satisfied: tf-models-official>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.13.2)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.7.1)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.20.3)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.33.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (3.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (9.4.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.84.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (3.0.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.5.16)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.8.0.76)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.5.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (6.0.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.11.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (0.1.99)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (0.7.5)\n",
            "Requirement already satisfied: tensorflow-text~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.13.0)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.1.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->mediapipe-model-maker) (2.13.3)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (1.4.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (4.66.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.10->mediapipe-model-maker) (0.41.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (6.0.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (3.16.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (0.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (4.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (6.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.4)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (1.15.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10->mediapipe-model-maker) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10->mediapipe-model-maker) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10->mediapipe-model-maker) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10->mediapipe-model-maker) (2.3.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (4.9)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (4.9.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (1.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->mediapipe-model-maker) (1.60.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (2.21)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (5.3.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.10->mediapipe-model-maker) (1.3.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow>=2.10->mediapipe-model-maker) (2.1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.10->mediapipe-model-maker) (3.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXvZgLPLh6n"
      },
      "source": [
        "次のコードを使用して、必要な Python クラスをインポートします。\n",
        "\n",
        "Use the following code to import the required Python classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oazmbPzKHYFq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import object_detector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "njsfJkI5DE6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e2f927-9efe-441d-8786-3605744de5dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O efficientdet.tflite -q https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/int8/1/efficientdet_lite0.tflite"
      ],
      "metadata": {
        "id": "IWTo9mPlTtNd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zP1AkaRL72Z"
      },
      "source": [
        "## データの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVnSV0ZQMA5-"
      },
      "source": [
        "オブジェクト検出用にモデルを再トレーニングするには、完成したモデルで識別できるようにするアイテムまたはクラスを含むデータセットが必要です。 これを行うには、パブリック データセットをトリミングしてユースケースに関連するクラスのみにするか、独自のデータセットをコンパイルするか、またはその両方を組み合わせます。データセットは、新しいモデルのトレーニングに必要なデータセットよりも大幅に小さくなる可能性があります。 たとえば、多くの参照モデルのトレーニングに使用される COCO データセットには、91 クラスのオブジェクトを含む数十万の画像が含まれています。 Model Maker を使用した転移学習は、推論精度の目標に応じて、より小さなデータセットを使用して既存のモデルを再トレーニングしても、良好なパフォーマンスを維持できます。 これらの手順では、2 種類のアンドロイド フィギュア、つまり 2 つのクラス、合計 62 個のトレーニング画像を含む小さなデータセットを使用します。\n",
        "\n",
        "サンプル データセットをダウンロードするには、次のコードを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Mz3-eHe07FEX"
      },
      "outputs": [],
      "source": [
        "train_dataset_path = \"/content/drive/MyDrive/xpFilesPNG/train/\"\n",
        "validation_dataset_path = \"/content/drive/MyDrive/xpFilesPNG/val\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEVcacUj7L1l"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbIAPjPUMHiK"
      },
      "source": [
        "### サポートされているデータセット形式\n",
        "Model Maker Object Detection API は、次のデータセット形式の読み取りをサポートしています。\n",
        "\n",
        "#### COCO 形式\n",
        "\n",
        "COCO データセット形式には、すべての画像を保存する「data」ディレクトリと、すべての画像のオブジェクト注釈を含む単一の「labels.json」ファイルがあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TPn8Mb_aJb"
      },
      "source": [
        "### データセットを確認する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L64U7mgPNKec"
      },
      "source": [
        "「labels.json」ファイルからカテゴリを出力して、データセットの内容を確認します。 カテゴリは合計 3 つあるはずです。 インデックス 0 は常に、データセット内で使用されない可能性がある「バックグラウンド」クラスに設定されます。 `android` と `pig_android` という 2 つの非バックグラウンド カテゴリがあるはずです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2f_Z-TAwNK3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16260c23-4e77-4f3b-c38f-08ffbfba07ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: MCP\n",
            "2: PIP\n",
            "3: DIP\n",
            "4: IP\n",
            "5: Wrist\n",
            "6: MCP1st\n"
          ]
        }
      ],
      "source": [
        "with open(os.path.join(train_dataset_path, \"labels.json\"), \"r\") as f:\n",
        "  labels_json = json.load(f)\n",
        "for category_item in labels_json[\"categories\"]:\n",
        "  print(f\"{category_item['id']}: {category_item['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr-7QJ05PmyS"
      },
      "source": [
        "データセットをより深く理解するには、いくつかのサンプル画像をその境界ボックスとともにプロットします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6kTw3uodPl7-"
      },
      "outputs": [],
      "source": [
        "#@title トレーニングデータの視覚化\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, text, patheffects\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "def draw_outline(obj):\n",
        "  obj.set_path_effects([patheffects.Stroke(linewidth=4,  foreground='black'), patheffects.Normal()])\n",
        "def draw_box(ax, bb):\n",
        "  patch = ax.add_patch(patches.Rectangle((bb[0],bb[1]), bb[2], bb[3], fill=False, edgecolor='red', lw=2))\n",
        "  draw_outline(patch)\n",
        "def draw_text(ax, bb, txt, disp):\n",
        "  text = ax.text(bb[0],(bb[1]-disp),txt,verticalalignment='top'\n",
        "  ,color='white',fontsize=10,weight='bold')\n",
        "  draw_outline(text)\n",
        "def draw_bbox(ax, annotations_list, id_to_label, image_shape):\n",
        "  for annotation in annotations_list:\n",
        "    cat_id = annotation[\"category_id\"]\n",
        "    bbox = annotation[\"bbox\"]\n",
        "    draw_box(ax, bbox)\n",
        "    draw_text(ax, bbox, id_to_label[cat_id], image_shape[0] * 0.05)\n",
        "def visualize(dataset_folder, max_examples=None):\n",
        "  with open(os.path.join(dataset_folder, \"labels.json\"), \"r\") as f:\n",
        "    labels_json = json.load(f)\n",
        "  images = labels_json[\"images\"]\n",
        "  cat_id_to_label = {item[\"id\"]:item[\"name\"] for item in labels_json[\"categories\"]}\n",
        "  image_annots = defaultdict(list)\n",
        "  for annotation_obj in labels_json[\"annotations\"]:\n",
        "    image_id = annotation_obj[\"image_id\"]\n",
        "    image_annots[image_id].append(annotation_obj)\n",
        "\n",
        "  if max_examples is None:\n",
        "    max_examples = len(image_annots.items())\n",
        "  n_rows = math.ceil(max_examples / 3)\n",
        "  fig, axs = plt.subplots(n_rows, 3, figsize=(24, n_rows*8)) # 3 columns(2nd index), 8x8 for each image\n",
        "  for ind, (image_id, annotations_list) in enumerate(list(image_annots.items())[:max_examples]):\n",
        "    ax = axs[ind//3, ind%3]\n",
        "    img = plt.imread(os.path.join(dataset_folder, \"images\", images[image_id][\"file_name\"]))\n",
        "    ax.imshow(img)\n",
        "    draw_bbox(ax, annotations_list, cat_id_to_label, img.shape)\n",
        "  plt.show()\n",
        "\n",
        "visualize(train_dataset_path, 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANqfl-ghQM74"
      },
      "source": [
        "### データセットを作成する\n",
        "\n",
        "Dataset クラスには、COCO または PASCAL VOC データセットをロードするための 2 つのメソッドがあります。\n",
        "* `Dataset.from_coco_folder`\n",
        "* `Dataset.from_pascal_voc_folder`\n",
        "\n",
        "android_figurines データセットは COCO データセット形式であるため、`from_coco_folder` メソッドを使用して、`train_dataset_path` と `validation_dataset_path` にあるデータセットを読み込みます。 データセットをロードすると、データは指定されたパスから解析され、標準化された [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) 形式に変換され、後で使用するためにキャッシュされます。 同じデータセットの複数のキャッシュを保存しないようにするには、`cache_dir` の場所を作成し、すべてのトレーニングでそれを再利用する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EOdyImqyI6s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a621e6-6f7f-4174-acdd-746e0f692646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data size:  411\n",
            "validation_data size:  46\n"
          ]
        }
      ],
      "source": [
        "train_data = object_detector.Dataset.from_coco_folder(train_dataset_path, cache_dir=\"/tmp/od_data/train\")\n",
        "validation_data = object_detector.Dataset.from_coco_folder(validation_dataset_path, cache_dir=\"/tmp/od_data/validation\")\n",
        "print(\"train_data size: \", train_data.size)\n",
        "print(\"validation_data size: \", validation_data.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "det3wYoWRRDs"
      },
      "source": [
        "## 再学習過程\n",
        "\n",
        "データの準備が完了したら、トレーニングデータで定義した新たな物体やクラスを認識するように再学習過程を開始することができる。以下の手順では、前のセクションで準備したデータを使用して画像分類モデルを再トレーニングし、2種類のアンドロイド フィギュアを認識する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9AzF_CGA7mj"
      },
      "source": [
        "### Set retraining options\n",
        "\n",
        "再トレーニングを実行するには、トレーニング データセットのほかに、モデルの出力ディレクトリとモデル アーキテクチャといういくつかの必要な設定があります。 HParams を使用して、出力ディレクトリの export_dir パラメータを指定します。 SupportedModels クラスを使用してモデル アーキテクチャを指定します。 物体検出ソリューションは、次のモデル アーキテクチャをサポートします。\n",
        "* `MobileNet-V2`\n",
        "* `MobileNet-MultiHW-AVG`\n",
        "トレーニング パラメーターのより高度なカスタマイズについては、以下の [ハイパーパラメーター](#hyperparameters) セクションをご覧ください。\n",
        "\n",
        "必要なパラメータを設定するには、次のコードを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4ZHjWHM1JyiN"
      },
      "outputs": [],
      "source": [
        "spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG\n",
        "hparams = object_detector.HParams(export_dir='exported_model')\n",
        "options = object_detector.ObjectDetectorOptions(\n",
        "    supported_model=spec,\n",
        "    hparams=hparams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Kto1RlCcPj"
      },
      "source": [
        "### 再学習過程\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V5bIsWBZCb8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a632f3-e582-4e12-fc55-a7d8c87dda26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing files at /tmp/model_maker/object_detector/mobilenetmultiavg\n",
            "Model: \"retina_net_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobile_net_1 (MobileNet)    {'2': (None, 64, 64, 32   3704416   \n",
            "                             ),                                  \n",
            "                              '3': (None, 32, 32, 64             \n",
            "                             ),                                  \n",
            "                              '4': (None, 16, 16, 16             \n",
            "                             0),                                 \n",
            "                              '5': (None, 8, 8, 192)             \n",
            "                             , '6': (None, 1, 1, 128             \n",
            "                             0)}                                 \n",
            "                                                                 \n",
            " fpn_1 (FPN)                 {'5': (None, 8, 8, 128)   144928    \n",
            "                             , '4': (None, 16, 16, 1             \n",
            "                             28),                                \n",
            "                              '3': (None, 32, 32, 12             \n",
            "                             8),                                 \n",
            "                              '6': (None, 4, 4, 128)             \n",
            "                             , '7': (None, 2, 2, 128             \n",
            "                             )}                                  \n",
            "                                                                 \n",
            " multilevel_detection_gener  multiple                  0 (unused)\n",
            " ator_1 (MultilevelDetectio                                      \n",
            " nGenerator)                                                     \n",
            "                                                                 \n",
            " retina_net_head_1 (RetinaN  ({'3': (None, 32, 32, 6   176867    \n",
            " etHead)                     3),                                 \n",
            "                              '4': (None, 16, 16, 63             \n",
            "                             ),                                  \n",
            "                              '5': (None, 8, 8, 63),             \n",
            "                              '6': (None, 4, 4, 63),             \n",
            "                              '7': (None, 2, 2, 63)}             \n",
            "                             , {'3': (None, 32, 32,              \n",
            "                             36),                                \n",
            "                              '4': (None, 16, 16, 36             \n",
            "                             ),                                  \n",
            "                              '5': (None, 8, 8, 36),             \n",
            "                              '6': (None, 4, 4, 36),             \n",
            "                              '7': (None, 2, 2, 36)}             \n",
            "                             , {})                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4026211 (15.36 MB)\n",
            "Trainable params: 3978147 (15.18 MB)\n",
            "Non-trainable params: 48064 (187.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2dbn_block_4/batch_normalization/gamma:0', 'conv2dbn_block_4/batch_normalization/beta:0', 'conv2dbn_block_5/batch_normalization/gamma:0', 'conv2dbn_block_5/batch_normalization/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2dbn_block_4/batch_normalization/gamma:0', 'conv2dbn_block_4/batch_normalization/beta:0', 'conv2dbn_block_5/batch_normalization/gamma:0', 'conv2dbn_block_5/batch_normalization/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2dbn_block_4/batch_normalization/gamma:0', 'conv2dbn_block_4/batch_normalization/beta:0', 'conv2dbn_block_5/batch_normalization/gamma:0', 'conv2dbn_block_5/batch_normalization/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2dbn_block_4/batch_normalization/gamma:0', 'conv2dbn_block_4/batch_normalization/beta:0', 'conv2dbn_block_5/batch_normalization/gamma:0', 'conv2dbn_block_5/batch_normalization/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 51s 263ms/step - total_loss: 2.4257 - cls_loss: 1.7336 - box_loss: 0.0126 - model_loss: 2.3628 - val_total_loss: 1.9422 - val_cls_loss: 1.1213 - val_box_loss: 0.0152 - val_model_loss: 1.8793\n",
            "Epoch 2/30\n",
            "51/51 [==============================] - 9s 166ms/step - total_loss: 1.3745 - cls_loss: 0.9623 - box_loss: 0.0070 - model_loss: 1.3116 - val_total_loss: 1.3904 - val_cls_loss: 0.8393 - val_box_loss: 0.0098 - val_model_loss: 1.3275\n",
            "Epoch 3/30\n",
            "51/51 [==============================] - 9s 170ms/step - total_loss: 0.8761 - cls_loss: 0.6038 - box_loss: 0.0042 - model_loss: 0.8133 - val_total_loss: 0.8879 - val_cls_loss: 0.5558 - val_box_loss: 0.0054 - val_model_loss: 0.8250\n",
            "Epoch 4/30\n",
            "51/51 [==============================] - 9s 168ms/step - total_loss: 0.6373 - cls_loss: 0.4247 - box_loss: 0.0030 - model_loss: 0.5745 - val_total_loss: 0.6689 - val_cls_loss: 0.4219 - val_box_loss: 0.0037 - val_model_loss: 0.6060\n",
            "Epoch 5/30\n",
            "51/51 [==============================] - 9s 172ms/step - total_loss: 0.5355 - cls_loss: 0.3498 - box_loss: 0.0025 - model_loss: 0.4727 - val_total_loss: 0.5457 - val_cls_loss: 0.3435 - val_box_loss: 0.0028 - val_model_loss: 0.4828\n",
            "Epoch 6/30\n",
            "51/51 [==============================] - 9s 170ms/step - total_loss: 0.4687 - cls_loss: 0.2987 - box_loss: 0.0021 - model_loss: 0.4059 - val_total_loss: 0.4751 - val_cls_loss: 0.2973 - val_box_loss: 0.0023 - val_model_loss: 0.4123\n",
            "Epoch 7/30\n",
            "51/51 [==============================] - 9s 172ms/step - total_loss: 0.4333 - cls_loss: 0.2713 - box_loss: 0.0020 - model_loss: 0.3704 - val_total_loss: 0.4017 - val_cls_loss: 0.2455 - val_box_loss: 0.0019 - val_model_loss: 0.3389\n",
            "Epoch 8/30\n",
            "51/51 [==============================] - 9s 171ms/step - total_loss: 0.4050 - cls_loss: 0.2497 - box_loss: 0.0019 - model_loss: 0.3422 - val_total_loss: 0.3870 - val_cls_loss: 0.2354 - val_box_loss: 0.0018 - val_model_loss: 0.3242\n",
            "Epoch 9/30\n",
            "51/51 [==============================] - 9s 172ms/step - total_loss: 0.3837 - cls_loss: 0.2324 - box_loss: 0.0018 - model_loss: 0.3210 - val_total_loss: 0.3632 - val_cls_loss: 0.2188 - val_box_loss: 0.0016 - val_model_loss: 0.3005\n",
            "Epoch 10/30\n",
            "51/51 [==============================] - 9s 172ms/step - total_loss: 0.3684 - cls_loss: 0.2195 - box_loss: 0.0017 - model_loss: 0.3056 - val_total_loss: 0.3476 - val_cls_loss: 0.2074 - val_box_loss: 0.0015 - val_model_loss: 0.2848\n",
            "Epoch 11/30\n",
            "51/51 [==============================] - 9s 169ms/step - total_loss: 0.3553 - cls_loss: 0.2101 - box_loss: 0.0017 - model_loss: 0.2926 - val_total_loss: 0.3389 - val_cls_loss: 0.2013 - val_box_loss: 0.0015 - val_model_loss: 0.2762\n",
            "Epoch 12/30\n",
            "51/51 [==============================] - 9s 176ms/step - total_loss: 0.3417 - cls_loss: 0.2005 - box_loss: 0.0016 - model_loss: 0.2790 - val_total_loss: 0.3328 - val_cls_loss: 0.1962 - val_box_loss: 0.0015 - val_model_loss: 0.2701\n",
            "Epoch 13/30\n",
            "51/51 [==============================] - 9s 169ms/step - total_loss: 0.3343 - cls_loss: 0.1941 - box_loss: 0.0016 - model_loss: 0.2716 - val_total_loss: 0.3228 - val_cls_loss: 0.1896 - val_box_loss: 0.0014 - val_model_loss: 0.2600\n",
            "Epoch 14/30\n",
            "51/51 [==============================] - 9s 172ms/step - total_loss: 0.3299 - cls_loss: 0.1909 - box_loss: 0.0015 - model_loss: 0.2672 - val_total_loss: 0.3153 - val_cls_loss: 0.1852 - val_box_loss: 0.0013 - val_model_loss: 0.2527\n",
            "Epoch 15/30\n",
            "51/51 [==============================] - 9s 174ms/step - total_loss: 0.3226 - cls_loss: 0.1856 - box_loss: 0.0015 - model_loss: 0.2599 - val_total_loss: 0.3101 - val_cls_loss: 0.1809 - val_box_loss: 0.0013 - val_model_loss: 0.2474\n",
            "Epoch 16/30\n",
            "51/51 [==============================] - 9s 176ms/step - total_loss: 0.3128 - cls_loss: 0.1796 - box_loss: 0.0014 - model_loss: 0.2501 - val_total_loss: 0.3048 - val_cls_loss: 0.1771 - val_box_loss: 0.0013 - val_model_loss: 0.2422\n",
            "Epoch 17/30\n",
            "51/51 [==============================] - 9s 173ms/step - total_loss: 0.3103 - cls_loss: 0.1765 - box_loss: 0.0014 - model_loss: 0.2476 - val_total_loss: 0.3009 - val_cls_loss: 0.1739 - val_box_loss: 0.0013 - val_model_loss: 0.2383\n",
            "Epoch 18/30\n",
            "51/51 [==============================] - 9s 174ms/step - total_loss: 0.3049 - cls_loss: 0.1731 - box_loss: 0.0014 - model_loss: 0.2423 - val_total_loss: 0.2961 - val_cls_loss: 0.1706 - val_box_loss: 0.0013 - val_model_loss: 0.2335\n",
            "Epoch 19/30\n",
            "51/51 [==============================] - 9s 171ms/step - total_loss: 0.2975 - cls_loss: 0.1675 - box_loss: 0.0013 - model_loss: 0.2349 - val_total_loss: 0.2933 - val_cls_loss: 0.1679 - val_box_loss: 0.0013 - val_model_loss: 0.2307\n",
            "Epoch 20/30\n",
            "51/51 [==============================] - 9s 171ms/step - total_loss: 0.3024 - cls_loss: 0.1692 - box_loss: 0.0014 - model_loss: 0.2398 - val_total_loss: 0.2895 - val_cls_loss: 0.1655 - val_box_loss: 0.0012 - val_model_loss: 0.2269\n",
            "Epoch 21/30\n",
            "51/51 [==============================] - 9s 173ms/step - total_loss: 0.2975 - cls_loss: 0.1654 - box_loss: 0.0014 - model_loss: 0.2349 - val_total_loss: 0.2863 - val_cls_loss: 0.1628 - val_box_loss: 0.0012 - val_model_loss: 0.2237\n",
            "Epoch 22/30\n",
            "51/51 [==============================] - 9s 168ms/step - total_loss: 0.2932 - cls_loss: 0.1631 - box_loss: 0.0014 - model_loss: 0.2306 - val_total_loss: 0.2819 - val_cls_loss: 0.1596 - val_box_loss: 0.0012 - val_model_loss: 0.2194\n",
            "Epoch 23/30\n",
            "51/51 [==============================] - 9s 177ms/step - total_loss: 0.2862 - cls_loss: 0.1587 - box_loss: 0.0013 - model_loss: 0.2236 - val_total_loss: 0.2778 - val_cls_loss: 0.1578 - val_box_loss: 0.0011 - val_model_loss: 0.2153\n",
            "Epoch 24/30\n",
            "51/51 [==============================] - 9s 167ms/step - total_loss: 0.2823 - cls_loss: 0.1568 - box_loss: 0.0013 - model_loss: 0.2198 - val_total_loss: 0.2786 - val_cls_loss: 0.1572 - val_box_loss: 0.0012 - val_model_loss: 0.2160\n",
            "Epoch 25/30\n",
            "51/51 [==============================] - 9s 171ms/step - total_loss: 0.2771 - cls_loss: 0.1530 - box_loss: 0.0012 - model_loss: 0.2146 - val_total_loss: 0.2749 - val_cls_loss: 0.1544 - val_box_loss: 0.0012 - val_model_loss: 0.2124\n",
            "Epoch 26/30\n",
            "51/51 [==============================] - 9s 170ms/step - total_loss: 0.2869 - cls_loss: 0.1584 - box_loss: 0.0013 - model_loss: 0.2244 - val_total_loss: 0.2717 - val_cls_loss: 0.1524 - val_box_loss: 0.0011 - val_model_loss: 0.2093\n",
            "Epoch 27/30\n",
            "51/51 [==============================] - 9s 175ms/step - total_loss: 0.2732 - cls_loss: 0.1510 - box_loss: 0.0012 - model_loss: 0.2107 - val_total_loss: 0.2708 - val_cls_loss: 0.1517 - val_box_loss: 0.0011 - val_model_loss: 0.2083\n",
            "Epoch 28/30\n",
            "51/51 [==============================] - 9s 172ms/step - total_loss: 0.2739 - cls_loss: 0.1506 - box_loss: 0.0012 - model_loss: 0.2114 - val_total_loss: 0.2701 - val_cls_loss: 0.1513 - val_box_loss: 0.0011 - val_model_loss: 0.2076\n",
            "Epoch 29/30\n",
            "51/51 [==============================] - 9s 171ms/step - total_loss: 0.2769 - cls_loss: 0.1514 - box_loss: 0.0013 - model_loss: 0.2145 - val_total_loss: 0.2662 - val_cls_loss: 0.1482 - val_box_loss: 0.0011 - val_model_loss: 0.2037\n",
            "Epoch 30/30\n",
            "51/51 [==============================] - 9s 175ms/step - total_loss: 0.2735 - cls_loss: 0.1486 - box_loss: 0.0012 - model_loss: 0.2111 - val_total_loss: 0.2633 - val_cls_loss: 0.1464 - val_box_loss: 0.0011 - val_model_loss: 0.2008\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.ObjectDetector.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuRapoFiRp34"
      },
      "source": [
        "### モデルのパフォーマンスを評価する\n",
        "\n",
        "モデルをトレーニングした後、検証データセットを用いてモデルを評価し、損失率と coco_metrics を出力します。 モデルのパフォーマンスを評価するための最も重要な指標は、通常、平均精度の「AP」ココメトリックです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xJvB_nf7RwzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c1b674-2d73-4ce9-a639-d182dd84ee5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 1s 43ms/step - total_loss: 0.2627 - cls_loss: 0.1461 - box_loss: 0.0011 - model_loss: 0.2003\n",
            "creating index...\n",
            "index created!\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.61s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.599\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.943\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.643\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.371\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.610\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.226\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.674\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.695\n",
            "Validation loss: [0.26270243525505066, 0.14606338739395142, 0.0010843955678865314, 0.20028315484523773]\n",
            "Validation coco metrics: {'AP': 0.5992683, 'AP50': 0.9431952, 'AP75': 0.6427812, 'APs': -1.0, 'APm': 0.37141603, 'APl': 0.6102862, 'ARmax1': 0.22558834, 'ARmax10': 0.67448467, 'ARmax100': 0.68074954, 'ARs': -1.0, 'ARm': 0.46005568, 'ARl': 0.6951534}\n"
          ]
        }
      ],
      "source": [
        "loss, coco_metrics = model.evaluate(validation_data, batch_size=4)\n",
        "print(f\"Validation loss: {loss}\")\n",
        "print(f\"Validation coco metrics: {coco_metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# ディレクトリの作成\n",
        "base_dir = \"/content/drive/MyDrive/xpFile_bbox\"\n",
        "if not os.path.exists(base_dir):\n",
        "    os.mkdir(base_dir)\n",
        "\n",
        "classes = ['MCP', 'PIP', 'DIP', 'IP', 'Wrist', 'MCP1st']\n",
        "for class_name in classes:\n",
        "    class_dir = os.path.join(base_dir, class_name)\n",
        "    if not os.path.exists(class_dir):\n",
        "        os.mkdir(class_dir)\n",
        "\n",
        "# bboxのクロップと保存\n",
        "for image_item in labels_json[\"images\"]:\n",
        "    image_path = os.path.join(train_dataset_path, \"images\", image_item[\"file_name\"])  # <--- ここを修正\n",
        "    with Image.open(image_path) as img:\n",
        "        for annotation_item in labels_json[\"annotations\"]:\n",
        "            if annotation_item[\"image_id\"] == image_item[\"id\"]:\n",
        "                class_id = annotation_item[\"category_id\"]\n",
        "                class_name = [category[\"name\"] for category in labels_json[\"categories\"] if category[\"id\"] == class_id][0]\n",
        "                bbox = annotation_item[\"bbox\"]  # [x,y,width,height]\n",
        "                cropped_img = img.crop((bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]))\n",
        "                cropped_img_name = f\"{annotation_item['id']}.png\"\n",
        "                cropped_img_path = os.path.join(base_dir, class_name, cropped_img_name)\n",
        "                cropped_img.save(cropped_img_path)\n",
        "\n",
        "print(\"All bbox images saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "qZmJjhbrhSVd",
        "outputId": "93906628-6310-4e7b-f779-ab61cb770594"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-5867697b7d8c>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mcropped_img_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{annotation_item['id']}.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mcropped_img_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_img_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mcropped_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All bbox images saved!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2426\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2428\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##検証のため、画像の出力を行う。\n",
        "\n"
      ],
      "metadata": {
        "id": "xgvg0varLClh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Boundingboxの境界線に沿って、関節部分の画像をクロップする。"
      ],
      "metadata": {
        "id": "1Bx1qi3kMiHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリのインポート\n",
        "from PIL import Image\n",
        "import os\n",
        "from PIL import ImageDraw\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/xpFile_bbox\"\n",
        "if not os.path.exists(base_dir):\n",
        "    os.mkdir(base_dir)\n",
        "\n",
        "# bboxをクロップして保存する関数\n",
        "def save_bbox_image(image_item, labels_json):\n",
        "    # ファイル名から必要な部分を取得\n",
        "    file_name = image_item[\"file_name\"]\n",
        "    dir_name = file_name[:6] + \"_\" + file_name[43:49]\n",
        "    dir_path = os.path.join(base_dir, dir_name)\n",
        "\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.mkdir(dir_path)\n",
        "\n",
        "    classes = ['MCP', 'PIP', 'DIP', 'IP', 'Wrist', 'MCP1st']\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(dir_path, class_name)\n",
        "        if not os.path.exists(class_dir):\n",
        "            os.mkdir(class_dir)\n",
        "\n",
        "    ann_image_dir = os.path.join(dir_path, \"annotationImage\")\n",
        "    if not os.path.exists(ann_image_dir):\n",
        "        os.mkdir(ann_image_dir)\n",
        "\n",
        "    image_path = os.path.join(train_dataset_path, \"images\", file_name)\n",
        "    with Image.open(image_path) as img:\n",
        "        drawed_img = img.copy()\n",
        "        draw = ImageDraw.Draw(drawed_img)\n",
        "\n",
        "        for annotation_item in labels_json[\"annotations\"]:\n",
        "            if annotation_item[\"image_id\"] == image_item[\"id\"]:\n",
        "                class_id = annotation_item[\"category_id\"]\n",
        "                class_name = [category[\"name\"] for category in labels_json[\"categories\"] if category[\"id\"] == class_id][0]\n",
        "                bbox = annotation_item[\"bbox\"]\n",
        "\n",
        "                cropped_img = img.crop((bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]))\n",
        "                cropped_img_name = f\"{dir_name}_{class_name}_{annotation_item['id']}.png\"\n",
        "                cropped_img_path = os.path.join(dir_path, class_name, cropped_img_name)\n",
        "                cropped_img.save(cropped_img_path)\n",
        "\n",
        "                # bboxを描画\n",
        "                draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], outline='red', width=2)\n",
        "\n",
        "        ann_image_name = f\"{dir_name}_annotation.png\"\n",
        "        ann_image_path = os.path.join(ann_image_dir, ann_image_name)\n",
        "        drawed_img.save(ann_image_path)\n",
        "\n",
        "# 各画像に対して関数を実行\n",
        "for image_item in labels_json[\"images\"]:\n",
        "    save_bbox_image(image_item, labels_json)\n",
        "\n",
        "print(\"All bbox images saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fPpvLTJaZw",
        "outputId": "86a318c8-904c-4123-b4aa-2b61458aa54c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All bbox images saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax8TkYA9VJUv"
      },
      "source": [
        "## モデルのエクスポート\n",
        "\n",
        "モデルを作成した後、後でデバイス上のアプリケーションで使用できるように、モデルを Tensorflow Lite モデル形式に変換してエクスポートします。 エクスポートには、ラベルマップを含むモデルメタデータも含まれます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWZqnGEKVP13"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model\n",
        "files.download('exported_model/model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcYu5ENbT4T6"
      },
      "source": [
        "## モデルの量子化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIeJjCfWTnBj"
      },
      "source": [
        "モデルを作成した後、後でデバイス上のアプリケーションで使用できるように、モデルを Tensorflow Lite モデル形式に変換してエクスポートします。 エクスポートにはモデルも含まれます。 モデル量子化は、比較的わずかな精度の低下のみでモデル サイズを削減し、予測速度を向上させることができるモデル変更手法です。\n",
        "\n",
        "ガイドのこのセクションでは、モデルに量子化を適用する方法について説明します。 Model Maker は、オブジェクト検出器の 2 つの形式の量子化をサポートしています。\n",
        "1. 量子化対応トレーニング: CPU 使用のための 8 ビット整数精度\n",
        "2. トレーニング後の量子化: GPU使用のための16 ビット浮動小数点精度。\n",
        "\n",
        "Model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy.\n",
        "\n",
        "This section of the guide explains how to apply quantization to your model. Model Maker supports two forms of quantization for object detector:\n",
        "1. Quantization Aware Training: 8 bit integer precision for CPU usage\n",
        "2. Post-Training Quantization: 16 bit floating point precision for GPU usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcB3DRfHWDfs"
      },
      "source": [
        "### 量子化を意識したトレーニング (int8 量子化)\n",
        "量子化対応トレーニング (QAT) は、モデルを完全にトレーニングした後に行われる微調整ステップです。 この手法では、8 ビット整数量子化の精度の低さを考慮して、推論時間量子化をエミュレートするモデルをさらに調整します。 標準 CPU を備えたオンデバイス アプリケーションの場合は、Int8 精度を使用します。 詳細については、[TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/training) ドキュメントを参照してください。\n",
        "\n",
        "量子化対応トレーニングを適用して int8 モデルにエクスポートするには、`QATHParams` 設定を作成し、`quantization_aware_training` メソッドを実行します。 `QATHParams` の詳細な使用法については、以下の **ハイパーパラメータ** セクションを参照してください。\n",
        "\n",
        "### Quantization aware training (int8 quantization)\n",
        "Quantization aware training (QAT) is a fine-tuning step which happens after fully training your model. This technique further tunes a model which emulates inference time quantization in order to account for the lower precision of 8 bit integer quantization. For on-device applications with a standard CPU, use Int8 precision. For more information, see the [TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/training) documentation.\n",
        "\n",
        "To apply quantization aware training and export to an int8 model, create a `QATHParams` configuration and run the `quantization_aware_training` method. See the **Hyperparameters** section below on detailed usage of `QATHParams`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7nRSQT9WCS-"
      },
      "outputs": [],
      "source": [
        "qat_hparams = object_detector.QATHParams(learning_rate=0.3, batch_size=4, epochs=10, decay_steps=6, decay_rate=0.96)\n",
        "model.quantization_aware_training(train_data, validation_data, qat_hparams=qat_hparams)\n",
        "qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n",
        "print(f\"QAT validation loss: {qat_loss}\")\n",
        "print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT7grgHOW048"
      },
      "source": [
        "QAT ステップでは、トレーニングのパラメーターを調整するために複数の実行が必要になることがよくあります。 `create` メソッドを使用してモデル トレーニングを再実行する必要がないようにするには、QAT を再度実行するために、`restore_float_ckpt` メソッドを使用してモデルの状態を完全にトレーニングされた浮動小数点モデルに戻します (`create` メソッドの実行後)。\n",
        "\n",
        "The QAT step often requires multiple runs to tune the parameters of training. To avoid having to rerun model training using the `create` method, use the `restore_float_ckpt` method to restore the model state back to the fully trained float model(After running the `create` method) in order to run QAT again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHDPWbIaXjR4"
      },
      "outputs": [],
      "source": [
        "new_qat_hparams = object_detector.QATHParams(learning_rate=0.9, batch_size=4, epochs=15, decay_steps=5, decay_rate=0.96)\n",
        "model.restore_float_ckpt()\n",
        "model.quantization_aware_training(train_data, validation_data, qat_hparams=new_qat_hparams)\n",
        "qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n",
        "print(f\"QAT validation loss: {qat_loss}\")\n",
        "print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfWo6TVpWJfr"
      },
      "source": [
        "最後に、export_model を使用して int8 量子化モデルにエクスポートします。 export_model 関数は、quantization_aware_training が実行されたかどうかに応じて、float32 モデルまたは int8 モデルのいずれかに自動的にエクスポートします。\n",
        "\n",
        "Finally, us the `export_model` to export to an int8 quantized model. The `export_model` function will automatically export to either float32 or int8 model depending on whether `quantization_aware_training` was run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsixePxCWJDp"
      },
      "outputs": [],
      "source": [
        "model.export_model('model_int8_qat.tflite')\n",
        "!ls -lh exported_model\n",
        "files.download('exported_model/model_int8_qat.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO8nZR3Cgx8_"
      },
      "source": [
        "### トレーニング後の量子化 (fp16 量子化)\n",
        "\n",
        "トレーニング後のモデルの量子化は、比較的わずかな精度の低下のみでモデルのサイズを削減し、予測速度を向上させることができるモデル変更手法です。 このアプローチでは、たとえば 32 ビット浮動小数点数を 16 ビット浮動小数点数に変換することにより、モデルによって処理されるデータのサイズが削減されます。 GPU の使用には Float16 量子化が推奨されます。 詳細については、[TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) ドキュメントを参照してください。\n",
        "\n",
        "まず、MediaPipe Model Maker 量子化モジュールをインポートします。\n",
        "\n",
        "### Post-training quantization (fp16 quantization)\n",
        "\n",
        "Post-training model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy. This approach reduces the size of the data processed by the model, for example by transforming 32-bit floating point numbers to 16-bit floats. Float16 quantization is reccomended for GPU usage. For more information, see the [TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) documentation.\n",
        "\n",
        "First, import the MediaPipe Model Maker quantization module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo7cQ_N-ZE8A"
      },
      "outputs": [],
      "source": [
        "from mediapipe_model_maker import quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE8j5cloZTo-"
      },
      "source": [
        "for_float16() クラス メソッドを使用して QuantizationConfig オブジェクトを定義します。 この構成は、32 ビット浮動小数点数の代わりに 16 ビット浮動小数点数を使用するようにトレーニングされたモデルを変更します。 QuantizationConfig クラスの追加パラメーターを設定することで、量子化プロセスをさらにカスタマイズできます。\n",
        "\n",
        "Define a QuantizationConfig object using the `for_float16()` class method. This configuration modifies a trained model to use 16-bit floating point numbers instead of 32-bit floating point numbers. You can further customize the quantization process by setting additional parameters for the QuantizationConfig class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbwc3g_Fa3dv"
      },
      "outputs": [],
      "source": [
        "quantization_config = quantization.QuantizationConfig.for_float16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTrkDXi8bM_L"
      },
      "source": [
        "追加の quantization_config オブジェクトを使用してモデルをエクスポートし、トレーニング後の量子化を適用します。 以前に quantization_aware_training を実行したことがある場合は、まず、restore_float_ckpt を使用してモデルを float モデルに変換し直す必要があることに注意してください。\n",
        "\n",
        "Export the model using the additional quantization_config object to apply post-training quantization. Note that if you previously ran `quantization_aware_training`, you must first convert the model back to a float model by using `restore_float_ckpt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmzEu_AjbMPI"
      },
      "outputs": [],
      "source": [
        "model.restore_float_ckpt()\n",
        "model.export_model(model_name=\"model_fp16.tflite\", quantization_config=quantization_config)\n",
        "!ls -lh exported_model\n",
        "files.download('exported_model/model_fp16.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npaRBUB3ZevY"
      },
      "source": [
        "##ハイパーパラメータ\n",
        "ObjectDetectorOptions クラスを使用してモデルをさらにカスタマイズできます。このクラスには、SupportedModels、ModelOptions、および HParams の 3 つのパラメーターがあります。\n",
        "\n",
        "SupportedModels enum クラスを使用して、トレーニングに使用するモデル アーキテクチャを指定します。 次のモデル アーキテクチャがサポートされています。\n",
        "\n",
        "モバイルネット_V2\n",
        "モバイルネット_V2_I320\n",
        "MOBILENET_MULTI_AVG\n",
        "MOBILENET_MULTI_AVG_I384\n",
        "HParams クラスを使用して、モデルのトレーニングと保存に関連する他のパラメーターをカスタマイズします。\n",
        "\n",
        "learning_rate: 勾配降下トレーニングに使用する学習率。 デフォルトは 0.3 です。\n",
        "batch_size: トレーニングのバッチ サイズ。 デフォルトは 8 です。\n",
        "エポック: データセットに対するトレーニング反復の数。 デフォルトは 30 です。\n",
        "cosine_decay_epochs: コサイン減衰学習率のエポック数。 詳細については、「tf.keras.optimizers.schedules.CosineDecay」を参照してください。 デフォルトは None で、これはエポックに設定するのと同じです。\n",
        "cosine_decay_alpha: コサイン減衰学習率のアルファ値。 詳細については、「tf.keras.optimizers.schedules.CosineDecay」を参照してください。 デフォルトは 1.0 で、コサイン減衰がないことを意味します。\n",
        "ModelOptions クラスを使用して、モデル自体に関連するパラメーターをカスタマイズします。\n",
        "\n",
        "l2_weight_decay: tf.keras.regulators.L2 で使用される L2 正則化ペナルティ。 デフォルトは 3e-5 です。\n",
        "QATHParams クラスを使用して、量子化対応トレーニングのトレーニング パラメーターをカスタマイズします。\n",
        "\n",
        "learning_rate: 勾配降下 QAT に使用する学習率。 デフォルトは 0.3 です。\n",
        "batch_size: QAT のバッチ サイズ。 デフォルトは 8\n",
        "エポック: データセットに対するトレーニング反復の数。 デフォルトは 15 です。\n",
        "decay_steps: 指数関数的減衰の学習率減衰ステップ。 詳細については、「tf.keras.optimizers.schedules.ExponentialDecay」を参照してください。 デフォルトは 8\n",
        "decay_rate: 指数関数的減衰の学習率減衰率。 詳細については、「tf.keras.optimizers.schedules.ExponentialDecay」を参照してください。 デフォルトは 0.96 です。\n",
        "\n",
        "## Hyperparameters\n",
        "You can further customize the model using the ObjectDetectorOptions class, which has three parameters for `SupportedModels`, `ModelOptions`, and `HParams`.\n",
        "\n",
        "Use the `SupportedModels` enum class to specify the model architecture to use for training. The following model architectures are supported:\n",
        "* MOBILENET_V2\n",
        "* MOBILENET_V2_I320\n",
        "* MOBILENET_MULTI_AVG\n",
        "* MOBILENET_MULTI_AVG_I384\n",
        "\n",
        "Use the `HParams` class to customize other parameters related to training and saving the model:\n",
        "* `learning_rate`: Learning rate to use for gradient descent training. Defaults to 0.3.\n",
        "* `batch_size`: Batch size for training. Defaults to 8.\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 30.\n",
        "* `cosine_decay_epochs`: The number of epochs for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to None, which is equivalent to setting it to `epochs`.\n",
        "* `cosine_decay_alpha`: The alpha value for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to 1.0, which means no cosine decay.\n",
        "\n",
        "Use the `ModelOptions` class to customize parameters related to the model itself:\n",
        "* `l2_weight_decay`: L2 regularization penalty used in [tf.keras.regularizers.L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2). Defaults to 3e-5.\n",
        "\n",
        "Uset the `QATHParams` class to customize training parameters for Quantization Aware Training:\n",
        "* `learning_rate`: Learning rate to use for gradient descent QAT. Defaults to 0.3.\n",
        "* `batch_size`: Batch size for QAT. Defaults to 8\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 15.\n",
        "* `decay_steps`: Learning rate decay steps for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 8\n",
        "* `decay_rate`: Learning rate decay rate for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 0.96."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HCrUl8z6liX"
      },
      "source": [
        "## ベンチマーク\n",
        "以下は、サポートされているモデル アーキテクチャのベンチマーク結果の概要です。 これらのモデルは、このノートブックと同じ Android フィギュア データセットでトレーニングおよび評価されました。 モデルのベンチマーク結果を検討する際には、留意すべき重要な注意点がいくつかあります。\n",
        "* Android フィギュア データセットは、62 個のトレーニング サンプルと 10 個の検証サンプルを含む小規模でシンプルなデータセットです。 データセットは非常に小さいため、トレーニング プロセスの差異によりメトリクスが大幅に変化する可能性があります。 このデータセットはデモ目的で提供されており、モデルのパフォーマンスを向上させるためにさらに多くのデータ サンプルを収集することをお勧めします。\n",
        "* float32 モデルはデフォルトの HParams でトレーニングされ、int8 モデルの QAT ステップは `QATHParams(learning_rate=0.1,batch_size=4,epochs=30,decay_rate=1)` で実行されました。\n",
        "* 独自のデータセットの場合、最良の結果を得るには、HParams と QATHParams の両方の値を調整する必要がある可能性があります。 トレーニング パラメーターの構成の詳細については、上記の [Hyperparameters](#hyperparameters) セクションを参照してください。\n",
        "* すべての遅延数値は Pixel 6 でベンチマークされています。\n",
        "\n",
        "## Benchmarking\n",
        "Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same android figurines dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n",
        "* The android figurines dataset is a small and simple dataset with 62 training examples and 10 validation examples. Since the dataset is quite small, metrics may vary drastically due to variances in the training process. This dataset was provided for demo purposes and it is recommended to collect more data samples for better performing models.\n",
        "* The float32 models were trained with the default HParams, and the QAT step for the int8 models was run with `QATHParams(learning_rate=0.1, batch_size=4, epochs=30, decay_rate=1)`.\n",
        "* For your own dataset, you will likely need to tune values for both HParams and QATHParams in order to achieve the best results. See the [Hyperparameters](#hyperparameters) section above for more information on configuring training parameters.\n",
        "* All latency numbers are benchmarked on the Pixel 6.\n",
        "\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<col>\n",
        "<col>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<tr>\n",
        "<th rowspan=\"2\">Model architecture</th>\n",
        "<th rowspan=\"2\">Input Image Size</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Test AP</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">CPU Latency</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Model Size</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNetV2</td>\n",
        "<td>256x256</td>\n",
        "<td>88.4%</td>\n",
        "<td>73.5%</td>\n",
        "<td>48ms</td>\n",
        "<td>16ms</td>\n",
        "<td>11MB</td>\n",
        "<td>3.2MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNetV2 I320</td>\n",
        "<td>320x320</td>\n",
        "<td>89.1%</td>\n",
        "<td>75.5%</td>\n",
        "<td>75ms</td>\n",
        "<td>33.38ms</td>\n",
        "<td>10MB</td>\n",
        "<td>3.3MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG</td>\n",
        "<td>256x256</td>\n",
        "<td>88.5%</td>\n",
        "<td>70.0%</td>\n",
        "<td>56ms</td>\n",
        "<td>19ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG I384</td>\n",
        "<td>384x384</td>\n",
        "<td>92.7%</td>\n",
        "<td>73.4%</td>\n",
        "<td>238ms</td>\n",
        "<td>41ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TOCPzKohXxy6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "c72243c5-0783-45d7-a5a4-f17c29dd2d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6c7447cca6a3>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mannotation_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mannotation_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mimage_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/xpFilesPNG/train/000067_1.2.392.200036.9107.307.13535.1353519020215409.121.png'"
          ]
        }
      ],
      "source": [
        "# これまでのコード\n",
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from mediapipe_model_maker import object_detector\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, text, patheffects\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "train_dataset_path = \"/content/drive/MyDrive/xpFilesPNG/train/\"\n",
        "validation_dataset_path = \"/content/drive/MyDrive/xpFilesPNG/val\"\n",
        "\n",
        "with open(os.path.join(train_dataset_path, \"labels.json\"), \"r\") as f:\n",
        "    labels_json = json.load(f)\n",
        "\n",
        "# Bboxの部分をクロップして保存するコード\n",
        "base_dir = \"/content/drive/MyDrive/xpFile_bbox\"\n",
        "if not os.path.exists(base_dir):\n",
        "    os.mkdir(base_dir)\n",
        "\n",
        "classes = ['MCP', 'PIP', 'DIP', 'IP', 'Wrist', 'MCP1st']\n",
        "for class_name in classes:\n",
        "    class_dir = os.path.join(base_dir, class_name)\n",
        "    if not os.path.exists(class_dir):\n",
        "        os.mkdir(class_dir)\n",
        "\n",
        "for image_item in labels_json[\"images\"]:\n",
        "    image_path = os.path.join(train_dataset_path, image_item[\"file_name\"])\n",
        "    with Image.open(image_path) as img:\n",
        "        for annotation_item in labels_json[\"annotations\"]:\n",
        "            if annotation_item[\"image_id\"] == image_item[\"id\"]:\n",
        "                class_id = annotation_item[\"category_id\"]\n",
        "                class_name = [category[\"name\"] for category in labels_json[\"categories\"] if category[\"id\"] == class_id][0]\n",
        "                bbox = annotation_item[\"bbox\"]  # [x,y,width,height]\n",
        "                cropped_img = img.crop((bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]))\n",
        "                cropped_img_name = f\"{annotation_item['id']}.png\"\n",
        "                cropped_img_path = os.path.join(base_dir, class_name, cropped_img_name)\n",
        "                cropped_img.save(cropped_img_path)\n",
        "\n",
        "train_data = object_detector.Dataset.from_coco_folder(train_dataset_path, cache_dir=\"/tmp/od_data/train\")\n",
        "validation_data = object_detector.Dataset.from_coco_folder(validation_dataset_path, cache_dir=\"/tmp/od_data/validation\")\n",
        "print(\"train_data size: \", train_data.size)\n",
        "print(\"validation_data size: \", validation_data.size)\n",
        "\n",
        "spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG\n",
        "hparams = object_detector.HParams(export_dir='exported_model')\n",
        "options = object_detector.ObjectDetectorOptions(\n",
        "    supported_model=spec,\n",
        "    hparams=hparams\n",
        ")\n",
        "model = object_detector.ObjectDetector.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}