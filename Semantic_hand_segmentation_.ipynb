{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTuCeHNddtVlvXEGdU36jg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyosukeHanaoka/TechTeacher/blob/main/Semantic_hand_segmentation_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pytorch を使用した、手のセマンティックセグメンテーション\n",
        "セマンティック セグメンテーションは、画像内の各ピクセルのクラスを予測するタスクです。この問題は、オブジェクトの周囲のボックスを予測する必要があるオブジェクト検出よりも困難です。これは、各ピクセルのクラスを予測するだけでなく、同じクラスの複数のインスタンスを区別する必要があるインスタンス セグメンテーションよりもわずかに簡単です。下の図は、取得しようとしている結果を示しています。\n",
        "\n",
        "ここでは、Pytorch とその事前定義モデルを使用して、手早く簡単に手のセグメンテーション ソフトウェアを立ち上げて実行してみます。\n",
        "\n",
        "独自のニューラル ネットワークを設計するのではなく、Pytorch のモデル リポジトリからResnet50 バックボーンを備えた DeepLabv3を使用します。次に、EGO Hands[2]、GTEA[3]、Hand over Face[1]データセットで構成される結合データセットでモデルをトレーニングします。これは、約 28,000 個の画像とそのセグメンテーション マスク (2.1 GB のデータ) を構成します。最後に、OpenCV を使用してリアルタイムで手をセグメント化するモデルを使用するいくつかの関数を作成します。"
      ],
      "metadata": {
        "id": "DeKEj2Xkr-Nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##モデル\n",
        "最初のステップは、Pytorch のリポジトリからモデルを取得することです。これは非常に簡単です。"
      ],
      "metadata": {
        "id": "6Mly7Zy8sJJu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Oiw4NraKq5Mz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "deeplab = models.segmentation.deeplabv3_resnet50(pretrained=0,\n",
        "                                                 progress=1,\n",
        "                                                 num_classes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは、torchvision のモデル モジュールを使用してdeeplabv3_resnet50モデルを取得します。2 つのグレースケール画像を生成するため、クラスの数をnum_classes2 として指定します。1 つは手のある領域を予測するため、もう 1 つは手のない領域を予測するためです。グレースケール画像は入力画像と同じサイズになります。これら 2 つの予測を比較して、モデルが画像の各ピクセルでハンドの可能性が高いか、ハンドが存在しないかを予測するかを確認します。"
      ],
      "metadata": {
        "id": "w9lldJtTsOWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に、データを処理するためのカスタム モデルを作成します。"
      ],
      "metadata": {
        "id": "pDvaglaosbFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HandSegModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HandSegModel,self).__init__()\n",
        "        self.dl = deeplab\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.dl(x)['out']\n",
        "        return y"
      ],
      "metadata": {
        "id": "Lfqn7TRjq-Lm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルに関して行うべきことはこれだけです。"
      ],
      "metadata": {
        "id": "ik4lRneVshco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##データ、データセット、データローダー\n",
        "次に、データを取得する必要があります。このリンクからデータを取得できます。フォルダーには 3 つのデータセットがあります。使用するデータセットのフォルダーを、Python コードを実行するフォルダーに配置する必要があります。あるいは、独自のデータセットを使用することもできます。😅\n",
        "\n",
        "データセットを作成し、Pytorch のデータローダーを使用してデータセットからバッチをフェッチします。これがカスタム データセットです。"
      ],
      "metadata": {
        "id": "pD_vY23HsnOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SegDataset(Dataset):\n",
        "\n",
        "    def __init__(self, parentDir, imageDir, maskDir):\n",
        "        self.imageList = glob.glob(parentDir+'/'+imageDir+'/*')\n",
        "        self.imageList.sort()\n",
        "        self.maskList = glob.glob(parentDir+'/'+maskDir+'/*')\n",
        "        self.maskList.sort()\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        preprocess = transforms.Compose([\n",
        "                                    transforms.Resize((384,288), 2),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "        X = Image.open(self.imageList[index]).convert('RGB')\n",
        "        X = preprocess(X)\n",
        "\n",
        "        trfresize = transforms.Resize((384, 288), 2)\n",
        "        trftensor = transforms.ToTensor()\n",
        "\n",
        "        yimg = Image.open(self.maskList[index]).convert('L')\n",
        "        y1 = trftensor(trfresize(yimg))\n",
        "        y1 = y1.type(torch.BoolTensor)\n",
        "        y2 = torch.bitwise_not(y1)\n",
        "        y = torch.cat([y2, y1], dim=0)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imageList)"
      ],
      "metadata": {
        "id": "EfIoLoXjrC9y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "マスクと画像が別のフォルダーにあり、それらのフォルダーが同じ親フォルダーにあると仮定します。\n",
        "コンストラクターは 3 つの引数を取ります。\n",
        "\n",
        "parentDir: イメージおよびマスク フォルダーが配置される親フォルダーの名前。\n",
        "imageDir: 画像が配置されているフォルダー。\n",
        "MaskDir: セグメンテーション マスクが配置されているフォルダー。\n",
        "クラスのコンストラクターで、SegDatasetイメージとマスクのファイル名のリストを取得します。この__getitem__ 関数では、画像とマスクの両方を取得します。画像のサイズを変更して標準化し、X を取得します。ラベルでもあるマスクについては、画像のサイズを変更して標準化します。その後、__bitwisenot__マスクに操作を適用して、元のマスクの正確なネガである別のマスクを取得します。次に、これら 2 つのマスクをスタックして 2 チャネル イメージを取得します。最初のチャネルはハンド ラベルに対応し、2 番目のチャネルはノーハンド ラベルに対応します。\n",
        "\n",
        "複数のデータセットを 1 つに結合する必要があります。これが私がやった方法です。"
      ],
      "metadata": {
        "id": "QWUr0dgCsxoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "EGOdataset = SegDataset('egodata', 'images', 'masks')\n",
        "HOFdataset = SegDataset('hand_over_face', 'images_resized', 'masks')\n",
        "GTEAdataset = SegDataset('GTEAhands', 'Images', 'Masks')\n",
        "\n",
        "megaDataset = ConcatDataset([EGOdataset, HOFdataset, GTEAdataset])"
      ],
      "metadata": {
        "id": "1NtLTtFVrHfW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それぞれに対して SegDataset を作成し、それらを .html として結合しますmegaDataset 💪。次に、データセットを分割し、トレーニング データセットと検証データセット用に 2 つのデータローダーを作成します。"
      ],
      "metadata": {
        "id": "fwZ_78ZStD67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#TTR is Train Test Ratio\n",
        "def trainTestSplit(dataset, TTR):\n",
        "    trainDataset = torch.utils.data.Subset(dataset, range(0, int(TTR * len(dataset))))\n",
        "    valDataset = torch.utils.data.Subset(dataset, range(int(TTR*len(dataset)), len(dataset)))\n",
        "    return trainDataset, valDataset\n",
        "\n",
        "batchSize = 2\n",
        "trainDataset, valDataset = trainTestSplit(megaDataset, 0.9)\n",
        "trainLoader = DataLoader(trainDataset, batch_size = batchSize, shuffle=True, drop_last=True)\n",
        "valLoader = DataLoader(valDataset, batch_size = batchSize, shuffle=True, drop_last=True)\n",
        "\n",
        "def trainTestSplit(dataset, TTR):\n",
        "    trainDataset = torch.utils.data.Subset(dataset, range(0, int(TTR * len(dataset))))\n",
        "    valDataset = torch.utils.data.Subset(dataset, range(int(TTR*len(dataset)), len(dataset)))\n",
        "    return trainDataset, valDataset\n",
        "\n",
        "batchSize = 2\n",
        "trainDataset, valDataset = trainTestSplit(megaDataset, 0.9)\n",
        "trainLoader = DataLoader(trainDataset, batch_size = batchSize, shuffle=True, drop_last=True)\n",
        "valLoader = DataLoader(valDataset, batch_size = batchSize, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "2kImoLV3rMx7",
        "outputId": "22d909ad-082a-437c-879e-5934ed3aa3cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-80ffcb3d41c0>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrainDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainTestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmegaDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainLoader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mvalLoader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    108\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(megaDataset))\n",
        "print(EGOdataset.imageList[:5])  # 最初の5つの画像のパスを表示\n",
        "print(EGOdataset.maskList[:5])   # 最初の5つのマスクのパスを表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pivIY-I1pja",
        "outputId": "e0ba0299-c11f-4059-ec09-baadffaf1313"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ピクセル精度は、正しく予測されたピクセルの数をピクセルの総数で割ったものです。"
      ],
      "metadata": {
        "id": "4r5qV4EXtNRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##パフォーマンス指標\n",
        "モデルとデータローダーの準備ができたので、トレーニング プロセスを追跡するためにいくつかのパフォーマンス メトリクスを作成します。\n",
        "\n",
        "このタスクのメトリクスとして、Intersection over UnionとPixel Accuracyを使用します。IOU は、セグメンテーション タスクの標準メトリックです。予測とターゲットの交差面積をそれらの結合で割った値が得られます。直観的には、正しく予測された領域の面積を関連する合計面積で割ったものと考えることができます。数値的には、0.5 を超える IOU 値が適切です。遠ければ遠いほど良いです。"
      ],
      "metadata": {
        "id": "zcNzGlNXtSNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def meanIOU(target, predicted):\n",
        "    if target.shape != predicted.shape:\n",
        "        print(\"target has dimension\", target.shape, \", predicted values have shape\", predicted.shape)\n",
        "        return\n",
        "\n",
        "    if target.dim() != 4:\n",
        "        print(\"target has dim\", target.dim(), \", Must be 4.\")\n",
        "        return\n",
        "\n",
        "    iousum = 0\n",
        "    for i in range(target.shape[0]):\n",
        "        target_arr = target[i, :, :, :].clone().detach().cpu().numpy().argmax(0)\n",
        "        predicted_arr = predicted[i, :, :, :].clone().detach().cpu().numpy().argmax(0)\n",
        "\n",
        "        intersection = np.logical_and(target_arr, predicted_arr).sum()\n",
        "        union = np.logical_or(target_arr, predicted_arr).sum()\n",
        "        if union == 0:\n",
        "            iou_score = 0\n",
        "        else :\n",
        "            iou_score = intersection / union\n",
        "        iousum +=iou_score\n",
        "\n",
        "    miou = iousum/target.shape[0]\n",
        "    return miou"
      ],
      "metadata": {
        "id": "m_lf5jkgrSYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ピクセル精度は、正しく予測されたピクセルの数をピクセルの総数で割ったものです。"
      ],
      "metadata": {
        "id": "33BDSGmItkAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pixelAcc(target, predicted):\n",
        "    if target.shape != predicted.shape:\n",
        "        print(\"target has dimension\", target.shape, \", predicted values have shape\", predicted.shape)\n",
        "        return\n",
        "\n",
        "    if target.dim() != 4:\n",
        "        print(\"target has dim\", target.dim(), \", Must be 4.\")\n",
        "        return\n",
        "\n",
        "    accsum=0\n",
        "    for i in range(target.shape[0]):\n",
        "        target_arr = target[i, :, :, :].clone().detach().cpu().numpy().argmax(0)\n",
        "        predicted_arr = predicted[i, :, :, :].clone().detach().cpu().numpy().argmax(0)\n",
        "\n",
        "        same = (target_arr == predicted_arr).sum()\n",
        "        a, b = target_arr.shape\n",
        "        total = a*b\n",
        "        accsum += same/total\n",
        "\n",
        "    pixelAccuracy = accsum/target.shape[0]\n",
        "    return pixelAccuracy"
      ],
      "metadata": {
        "id": "pBkHcEYsrVy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##トレーニング\n",
        "さて、私たち全員が待ち望んでいたトレーニングの瞬間です。\n",
        "\n",
        "まず、適切なハイパーパラメータを使用して、オプティマイザと損失関数、モデル、学習率スケジューラ オブジェクトを作成します。"
      ],
      "metadata": {
        "id": "JCUY-sBQteJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = HandSegModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
        "loss_fn = nn.BCEWithLogitsLoss ()\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.8)"
      ],
      "metadata": {
        "id": "Pq-UNt9irY7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に、作成されたすべてのオブジェクトを引数として受け取り、モデルをトレーニングするトレーニング ループ関数を作成します。"
      ],
      "metadata": {
        "id": "BwTzs5jsunFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, lr_scheduler, model, loss_fn, train_loader, val_loader, lastCkptPath = None):\n",
        "    if torch.cuda.is_available():\n",
        "        dev = \"cuda:0\"\n",
        "    else:\n",
        "        dev = \"cpu\"\n",
        "    device = torch.device(dev)\n",
        "\n",
        "    tr_loss_arr = []\n",
        "    val_loss_arr = []\n",
        "    meanioutrain = []\n",
        "    pixelacctrain = []\n",
        "    meanioutest = []\n",
        "    pixelacctest = []\n",
        "    prevEpoch = 0\n",
        "\n",
        "    if lastCkptPath != None :\n",
        "        checkpoint = torch.load(lastCkptPath)\n",
        "        prevEpoch = checkpoint['epoch']model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])for state in optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    state[k] = v.to(device)tr_loss_arr =  checkpoint['Training Loss']\n",
        "        val_loss_arr =  checkpoint['Validation Loss']\n",
        "        meanioutrain =  checkpoint['MeanIOU train']\n",
        "        pixelacctrain =  checkpoint['PixelAcc train']\n",
        "        meanioutest =  checkpoint['MeanIOU test']\n",
        "        pixelacctest =  checkpoint['PixelAcc test']\n",
        "        print(\"loaded model, \", checkpoint['description'], \"at epoch\", prevEpoch)model.to(device)\n",
        "\n",
        "    for epoch in range(0, n_epochs):\n",
        "        train_loss = 0.0\n",
        "        pixelacc = 0\n",
        "        meaniou = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, total = len(train_loader))\n",
        "        for X, y in pbar:\n",
        "            torch.cuda.empty_cache()\n",
        "            model.train()\n",
        "            X = X.to(device).float()\n",
        "            y = y.to(device).float()\n",
        "            ypred = model(X)\n",
        "            loss = loss_fn(ypred, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tr_loss_arr.append(loss.item())\n",
        "            meanioutrain.append(meanIOU(y, ypred))\n",
        "            pixelacctrain.append(pixelAcc(y, ypred))\n",
        "            pbar.set_postfix({'Epoch':epoch+1+prevEpoch,\n",
        "                              'Training Loss': np.mean(tr_loss_arr),\n",
        "                              'Mean IOU': np.mean(meanioutrain),\n",
        "                              'Pixel Acc': np.mean(pixelacctrain)\n",
        "                             })\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            val_loss = 0\n",
        "            pbar = tqdm(val_loader, total = len(val_loader))\n",
        "            for X, y in pbar:\n",
        "                torch.cuda.empty_cache()\n",
        "                X = X.to(device).float()\n",
        "                y = y.to(device).float()\n",
        "                model.eval()\n",
        "                ypred = model(X)\n",
        "\n",
        "                val_loss_arr.append(loss_fn(ypred, y).item())\n",
        "                pixelacctest.append(pixelAcc(y, ypred))\n",
        "                meanioutest.append(meanIOU(y, ypred))\n",
        "\n",
        "                pbar.set_postfix({'Epoch':epoch+1+prevEpoch,\n",
        "                                  'Validation Loss': np.mean(val_loss_arr),\n",
        "                                  'Mean IOU': np.mean(meanioutest),\n",
        "                                  'Pixel Acc': np.mean(pixelacctest)\n",
        "                                 })\n",
        "\n",
        "\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch':epoch+1+prevEpoch,\n",
        "            'description':\"add your description\",\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'Training Loss': tr_loss_arr,\n",
        "            'Validation Loss':val_loss_arr,\n",
        "            'MeanIOU train':meanioutrain,\n",
        "            'PixelAcc train':pixelacctrain,\n",
        "            'MeanIOU test':meanioutest,\n",
        "            'PixelAcc test':pixelacctest\n",
        "        }\n",
        "        torch.save(checkpoint, 'checkpoints/checkpointhandseg'+str(epoch+1+prevEpoch)+'.pt')\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return tr_loss_arr, val_loss_arr, meanioutrain, pixelacctrain, meanioutest, pixelacctest"
      ],
      "metadata": {
        "id": "Zc23A_Lrrgvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "トレーニング中の平均損失とパフォーマンス メトリクスが表示されます。エポックごとに、これまでに計算されたすべての損失とメトリクスを含むチェックポイントが保存されます。これにより、いつでもトレーニングを停止し、データを失うことなく再開できるようになります。この関数の最後の引数は、トレーニングを再開するチェックポイントへのパスです。この関数は、トレーニングからのすべての重要なデータ、つまりトレーニングおよび検証データセット上の損失とメトリクスのタプルを返します。\n",
        "\n",
        "まず、トレーニング ループを実行します。"
      ],
      "metadata": {
        "id": "lxkZ3hbiu23r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#call the training loop,\n",
        "#make sure to pass correct checkpoint path, or none if starting with the training\n",
        "\n",
        "retval = training_loop(3,\n",
        "                       optimizer,\n",
        "                       lr_scheduler,\n",
        "                       model,\n",
        "                       loss_fn,\n",
        "                       trainLoader,\n",
        "                       valLoader,\n",
        "                       'checkpoints/checkpointhandseg.pt')"
      ],
      "metadata": {
        "id": "K3e2-wf-rk8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データをプロットするには、リストの移動平均を使用できます。"
      ],
      "metadata": {
        "id": "tUyxgbxSu9Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#after the training loop returns, we can plot the data\n",
        "def running_mean(x, N):\n",
        "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
        "\n",
        "fig, ax = plt.subplots(ncols = 3, nrows = 2, figsize = (20,10))\n",
        "N = 1000\n",
        "ax[0][0].plot(running_mean(retval[0], N), 'r.', label='training loss')\n",
        "ax[1][0].plot(running_mean(retval[1], N), 'b.', label='validation loss')\n",
        "ax[0][1].plot(running_mean(retval[2], N), 'g.', label='meanIOU training')\n",
        "ax[1][1].plot(running_mean(retval[4], N), 'r.', label='meanIOU validation')\n",
        "ax[0][2].plot(running_mean(retval[3], N), 'b.', label='pixelAcc  training')\n",
        "ax[1][2].plot(running_mean(retval[5], N), 'b.', label='pixelAcc validation')\n",
        "for i in ax:\n",
        "    for j in i:\n",
        "        j.legend()\n",
        "        j.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PPyCfaNLrolG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "N 値を少し調整して、より滑らかな、またはよりノイズの多いプロットを取得してみることができます。"
      ],
      "metadata": {
        "id": "BpM_oyKXvGwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##リアルタイムの手のセグメンテーション\n",
        "私たちは自分たちの努力の結果を見る準備ができています。OpenCV を使用してカメラからフレームを読み取り、フレームのセグメンテーション マスクを予測します。\n",
        "\n",
        "まず、モデルから予測を取得するためのヘルパー関数をいくつか作成します。"
      ],
      "metadata": {
        "id": "LphtggN9vIvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#can pass np array or path to image file\n",
        "def SegmentHands(pathtest):\n",
        "\n",
        "    if isinstance(pathtest, np.ndarray):\n",
        "        img = Image.fromarray(pathtest)\n",
        "    else :\n",
        "        img = Image.open(pathtest)\n",
        "\n",
        "    preprocess = transforms.Compose([transforms.Resize((288, 384), 2),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "    Xtest = preprocess(img)\n",
        "\n",
        "    checkpoint = torch.load('checkpoints/checkpointhandseg7.pt')\n",
        "    model = HandSegModel()\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        if torch.cuda.is_available():\n",
        "            dev = \"cuda:0\"\n",
        "        else:\n",
        "            dev = \"cpu\"\n",
        "        device = torch.device(dev)\n",
        "        model.to(device)\n",
        "        Xtest = Xtest.to(device).float()\n",
        "        ytest = model(Xtest.unsqueeze(0).float())\n",
        "        ypos = ytest[0, 1, :, :].clone().detach().cpu().numpy()\n",
        "        yneg = ytest[0, 0, :, :].clone().detach().cpu().numpy()\n",
        "        ytest = ypos >= yneg\n",
        "\n",
        "    mask = ytest.astype('float32')\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n",
        "    mask = cv2.dilate(mask,kernel,iterations = 2)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "bal4oxOErst7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この関数はバイナリマスクを出力します。入力として画像の numpy 配列または画像のパスを受け取ります。画像に閉じる操作と開く操作を適用して、ノイズを軽減し、マスクを少し拡張します。\n",
        "\n",
        "次に、画像内の手の領域に重み付けされた緑色のマスクを追加する関数がいくつかあります。"
      ],
      "metadata": {
        "id": "pSwSy3sFvSM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getcoloredMask(image, mask):\n",
        "    color_mask = np.zeros_like(image)\n",
        "    color_mask[:, :, 1] += mask.astype('uint8') * 250\n",
        "    masked = cv2.addWeighted(image, 1.0, color_mask, 1.0, 0.0)\n",
        "    return masked"
      ],
      "metadata": {
        "id": "KAgZTXn8rv03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に、OpenCV を使用してカメラから画像を読み取り、画像内の手の領域を予測します。"
      ],
      "metadata": {
        "id": "ZObybbMuvbW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "i = 0\n",
        "while(True):\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    frame = cv2.resize(frame, (384,288))\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    if i%5 == 0:\n",
        "        i=0\n",
        "        mask = SegmentHands(rgb)\n",
        "        colmask = getcoloredMask(frame, mask)\n",
        "\n",
        "    cv2.imshow('color', np.hstack((frame, colmask)))\n",
        "    key = cv2.waitKey(24)\n",
        "    if key & 0xFF == ord('q'):\n",
        "        break\n",
        "    i+=1\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "eey7PUv7r05H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##その他に試してみること:\n",
        "\n",
        "データ拡張（ランダム クロッピング、スケーリング、ノイズ追加など）を使用します。\n",
        "別のモデル アーキテクチャを使用する\n",
        "より大きな、またはより強力なデータセットを使用する\n",
        "ハイパーパラメータを調整する"
      ],
      "metadata": {
        "id": "Yp_C7nTDvk1q"
      }
    }
  ]
}